{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Group 1 in Practical Planning Robust Behavior for autonomous driving\n",
    "# Reinforcement Learning using Graph Neural Networks\n",
    "\n",
    "### Tom DÃ¶rr, Marco Oliva, Quoc Trung Nguyen, Silvan Wimmer\n",
    "\n",
    "__Objective__: Implement an reinforcement learning (RL) approach to train a graph neural network (GNN) in the setting of autonomous driving.\n",
    "## Chapter 0: Setting up\n",
    "### 0.0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "import os\n",
    "import networkx as nx\n",
    "import time\n",
    "#import json\n",
    "#import pickle\n",
    "#import logging\n",
    "from collections import OrderedDict\n",
    "from matplotlib.patches import Ellipse\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# BARK imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "from bark.runtime.viewer.matplotlib_viewer import MPViewer\n",
    "from bark.runtime.viewer.video_renderer import VideoRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BARK-ML imports\n",
    "from bark_ml.environments.blueprints import ContinuousHighwayBlueprint, \\\n",
    "  ContinuousMergingBlueprint, ContinuousIntersectionBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf_agents.agents import BehaviorSACAgent, BehaviorPPOAgent, BehaviorGraphSACAgent\n",
    "#from bark_ml.library_wrappers.lib_tf_agents.runners import SACRunner, PPORunner\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "#from bark_ml.library_wrappers.lib_tf2_gnn import GNNActorNetwork, GNNCriticNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised_learning.data_generation import DataGenerator\n",
    "from supervised_learning.data_handler import Dataset\n",
    "from supervised_learning.learner import Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Local variables for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_ref_highway = OrderedDict([('x', [5112.68310546875, 5119.88330078125]),\n",
    "                                         ('y', [5054.984375, 5304.984375]),\n",
    "                                         ('theta', [0, 6.283185307179586]),\n",
    "                                         ('vel', [0, 100]),\n",
    "                                         ('distance', [0, 250.10366413257154]),\n",
    "                                         ('dx', [-7.2001953125, 7.2001953125]),\n",
    "                                         ('dy', [-250.0, 250.0])])\n",
    "\n",
    "params_path = os.path.join(\"data\", \"tfa_params.json\")\n",
    "params = ParameterServer(filename=params_path)\n",
    "params[\"World\"][\"remove_agents_out_of_map\"] = False\n",
    "data_path = os.path.join(\"data\")\n",
    "\n",
    "visible_distance = params[\"ML\"][\"GraphObserver\"][\"VisibilityRadius\", \"\", 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/silvan/working_bark'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bark_path = os.path.join(\"/home\", \"silvan\", \"working_bark\")\n",
    "bark_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Local functions (will be transfered to helper_functions.py, but easier prototyping here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docs.report.helper_functions import visualize_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Reinforcement learning setting\n",
    "- what learning setting we have (observation, action, reward)\n",
    "- SAC approach with actor net outputting distributions of actions and critic net\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space has shape (127,) \n",
      "with minimum value \n",
      "[ 0.  0.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.] \n",
      "and maximum value \n",
      "[100. 100. 100.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "   1.]\n",
      "Action space has shape (2,)\n",
      " with minimum value \n",
      "[-5.  -0.2]\n",
      "and maximum value \n",
      "[4.  0.2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAFDCAYAAAB/UdRdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADxVJREFUeJzt3F+o5Hd5x/HPY2Iq1ailWUGyiUnpWl20oD2EFKGmaEuSi82FrSQgVgku2EZKFSHFEiVeWakFIa1uqVgFjdELWXBLLmwkIEayYg0mIbKN1mwUsv7LTdCY9unFGctx3c2ZbOY865y8XnBgfr/5npmHL4fz3pkz+6vuDgCw8551tgcAgGcK0QWAIaILAENEFwCGiC4ADBFdABiybXSr6mNV9UhVffM091dVfbiqjlXVPVX16tWPCQDrb5lXuh9PcuWT3H9Vkn2Lr4NJ/vnpjwUAu8+20e3uO5P86EmWXJPkE73priQvrKoXr2pAANgtVvE33QuTPLTl+PjiHACwxbmTT1ZVB7P5FnSe+9zn/sHLXvayyacHgKfta1/72g+6e8+ZfO8qovtwkou2HO9dnPsV3X0oyaEk2djY6KNHj67g6QFgTlX995l+7yreXj6c5M2LTzFfnuTR7v7+Ch4XAHaVbV/pVtWnk1yR5IKqOp7kvUmenSTd/ZEkR5JcneRYkseSvHWnhgWAdbZtdLv7um3u7yR/tbKJAGCXckUqABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABiyVHSr6sqqeqCqjlXVjae4/+KquqOqvl5V91TV1asfFQDW27bRrapzktyS5Kok+5NcV1X7T1r2d0lu6+5XJbk2yT+telAAWHfLvNK9LMmx7n6wux9PcmuSa05a00mev7j9giTfW92IALA7LBPdC5M8tOX4+OLcVu9L8qaqOp7kSJJ3nOqBqupgVR2tqqMnTpw4g3EBYH2t6oNU1yX5eHfvTXJ1kk9W1a88dncf6u6N7t7Ys2fPip4aANbDMtF9OMlFW473Ls5tdX2S25Kku7+S5DlJLljFgACwWywT3buT7KuqS6vqvGx+UOrwSWu+m+R1SVJVL89mdL1/DABbbBvd7n4iyQ1Jbk9yfzY/pXxvVd1cVQcWy96V5G1V9Y0kn07ylu7unRoaANbRucss6u4j2fyA1NZzN225fV+S16x2NADYXVyRCgCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAEOWim5VXVlVD1TVsaq68TRr3lhV91XVvVX1qdWOCQDr79ztFlTVOUluSfInSY4nubuqDnf3fVvW7Evyt0le090/rqoX7dTAALCulnmle1mSY939YHc/nuTWJNectOZtSW7p7h8nSXc/stoxAWD9LRPdC5M8tOX4+OLcVi9N8tKq+nJV3VVVV65qQADYLbZ9e/kpPM6+JFck2Zvkzqp6ZXf/ZOuiqjqY5GCSXHzxxSt6agBYD8u80n04yUVbjvcuzm11PMnh7v55d387ybeyGeFf0t2Hunujuzf27NlzpjMDwFpaJrp3J9lXVZdW1XlJrk1y+KQ1n8/mq9xU1QXZfLv5wRXOCQBrb9vodvcTSW5IcnuS+5Pc1t33VtXNVXVgsez2JD+sqvuS3JHk3d39w50aGgDWUXX3WXnijY2NPnr06Fl5bgA4U1X1te7eOJPvdUUqABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABgiugAwRHQBYIjoAsAQ0QWAIaILAENEFwCGiC4ADBFdABiyVHSr6sqqeqCqjlXVjU+y7g1V1VW1sboRAWB32Da6VXVOkluSXJVkf5Lrqmr/Kdadn+Svk3x11UMCwG6wzCvdy5Ic6+4Hu/vxJLcmueYU696f5ANJfrrC+QBg11gmuhcmeWjL8fHFuf9XVa9OclF3f2GFswHArvK0P0hVVc9K8qEk71pi7cGqOlpVR0+cOPF0nxoA1soy0X04yUVbjvcuzv3C+UlekeRLVfWdJJcnOXyqD1N196Hu3ujujT179pz51ACwhpaJ7t1J9lXVpVV1XpJrkxz+xZ3d/Wh3X9Ddl3T3JUnuSnKgu4/uyMQAsKa2jW53P5HkhiS3J7k/yW3dfW9V3VxVB3Z6QADYLc5dZlF3H0ly5KRzN51m7RVPfywA2H1ckQoAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhiwV3aq6sqoeqKpjVXXjKe5/Z1XdV1X3VNUXq+olqx8VANbbttGtqnOS3JLkqiT7k1xXVftPWvb1JBvd/ftJPpfk71c9KACsu2Ve6V6W5Fh3P9jdjye5Nck1Wxd09x3d/dji8K4ke1c7JgCsv2Wie2GSh7YcH1+cO53rk/z7qe6oqoNVdbSqjp44cWL5KQFgF1jpB6mq6k1JNpJ88FT3d/eh7t7o7o09e/as8qkB4NfeuUuseTjJRVuO9y7O/ZKqen2S9yR5bXf/bDXjAcDuscwr3buT7KuqS6vqvCTXJjm8dUFVvSrJR5Mc6O5HVj8mAKy/baPb3U8kuSHJ7UnuT3Jbd99bVTdX1YHFsg8meV6Sz1bVf1bV4dM8HAA8Yy3z9nK6+0iSIyedu2nL7deveC4A2HVckQoAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDRBcAhoguAAwRXQAYIroAMER0AWCI6ALAENEFgCGiCwBDlopuVV1ZVQ9U1bGquvEU9/9GVX1mcf9Xq+qSVQ8KAOtu2+hW1TlJbklyVZL9Sa6rqv0nLbs+yY+7+3eT/GOSD6x6UABYd8u80r0sybHufrC7H09ya5JrTlpzTZJ/W9z+XJLXVVWtbkwAWH/LRPfCJA9tOT6+OHfKNd39RJJHk/z2KgYEgN3i3Mknq6qDSQ4uDn9WVd+cfP5niAuS/OBsD7FL2dudYV93jr3dGb93pt+4THQfTnLRluO9i3OnWnO8qs5N8oIkPzz5gbr7UJJDSVJVR7t740yG5vTs686xtzvDvu4ce7szquromX7vMm8v351kX1VdWlXnJbk2yeGT1hxO8heL23+W5D+6u890KADYjbZ9pdvdT1TVDUluT3JOko91971VdXOSo919OMm/JvlkVR1L8qNshhkA2GKpv+l295EkR046d9OW2z9N8udP8bkPPcX1LMe+7hx7uzPs686xtzvjjPe1vAsMADNcBhIAhux4dF1Ccmcssa/vrKr7quqeqvpiVb3kbMy5jrbb2y3r3lBVXVU+HbqEZfa1qt64+Lm9t6o+NT3jOlrid8HFVXVHVX198fvg6rMx57qpqo9V1SOn+6+ttenDi32/p6pevdQDd/eOfWXzg1f/leR3kpyX5BtJ9p+05i+TfGRx+9okn9nJmXbD15L7+sdJfnNx++32dXV7u1h3fpI7k9yVZONsz/3r/rXkz+y+JF9P8luL4xed7bl/3b+W3NdDSd6+uL0/yXfO9tzr8JXkj5K8Osk3T3P/1Un+PUkluTzJV5d53J1+pesSkjtj233t7ju6+7HF4V3Z/P/VbG+Zn9kkeX82rzH+08nh1tgy+/q2JLd094+TpLsfGZ5xHS2zr53k+YvbL0jyvcH51lZ335nN/41zOtck+URvuivJC6vqxds97k5H1yUkd8Yy+7rV9dn8Fxnb23ZvF28jXdTdX5gcbM0t8zP70iQvraovV9VdVXXl2HTra5l9fV+SN1XV8Wz+L5R3zIy26z3V38NJhi8DybyqelOSjSSvPduz7AZV9awkH0rylrM8ym50bjbfYr4im+/M3FlVr+zun5zVqdbfdUk+3t3/UFV/mM1rKryiu//3bA/2TLTTr3SfyiUk82SXkOSXLLOvqarXJ3lPkgPd/bOh2dbddnt7fpJXJPlSVX0nm3/LOezDVNta5mf2eJLD3f3z7v52km9lM8Kc3jL7en2S25Kku7+S5DnZvCYzT89Sv4dPttPRdQnJnbHtvlbVq5J8NJvB9bex5T3p3nb3o919QXdf0t2XZPPv5Qe6+4yvxfoMsczvgs9n81VuquqCbL7d/ODkkGtomX39bpLXJUlVvTyb0T0xOuXudDjJmxefYr48yaPd/f3tvmlH315ul5DcEUvu6weTPC/JZxefS/tudx84a0OviSX3lqdoyX29PcmfVtV9Sf4nybu727teT2LJfX1Xkn+pqr/J5oeq3uKFzfaq6tPZ/EfgBYu/h783ybOTpLs/ks2/j1+d5FiSx5K8danHtfcAMMMVqQBgiOgCwBDRBYAhogsAQ0QXAIaILgAMEV0AGCK6ADDk/wCYzYqtOC7aMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_generator = DataGenerator(num_scenarios=3, dump_dir=data_path, params=params)\n",
    "print('Observation space has shape {} \\nwith minimum value \\n{} \\nand maximum value \\n{}'.format(graph_generator.env.observation_space.shape, graph_generator.env.observation_space.low, graph_generator.env.observation_space.high))\n",
    "print('Action space has shape {}\\n with minimum value \\n{}\\nand maximum value \\n{}'.format(graph_generator.env.action_space.shape, graph_generator.env.action_space.low, graph_generator.env.action_space.high))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reinforcement learning problem, the observation of environment has size of **3**+**agent_limit** x **feature_len** + size(**adjacency_matrix**), in which: \n",
    "- **agent_limit** is the maximum number of agents that can be observed\n",
    "- **feature_len** is number feature per agent \n",
    "- **adjacency_matrix** represent relation between one agent with other agents, which has size **agent_limit** x **agent_limit** \n",
    "- the first three components are maximum number of agents, actual number of agents and number of feature per agent\n",
    "\n",
    "In this case, **agent_limmit** = 12 and **feature_len** = 11\n",
    "\n",
    "The action has size of 2, which represent the steering and acceleration of agent, respectively\n",
    "\n",
    "The reward is assigned as follows: \n",
    " - If the agent reaches goal, it receives reward of 1\n",
    " - If the agent crashes, it receives reward of -1\n",
    " - For each time step that the agent does not crash, it receives a reward of 0.01\n",
    " \n",
    "Our project implements the Soft Actor Critic algorithm for solving the reinforement learning problem. It consists of two different network: \n",
    "  - The **Critic network** receives observation and action as inputs and tries to approximate the action-value function $Q_{\\phi_i}(s,a)$ to the true action-value function $Q^{\\pi}(s,a)$. More specifically, it tries to minimize the error between approximated and target value function\n",
    "  \\begin{equation}\n",
    "  L(\\phi_i, {\\mathcal D}) = \\underset{(s,a,r,s',d) \\sim {\\mathcal D}}{{\\mathrm E}}\\left[\n",
    "    \\Bigg( Q_{\\phi_i}(s,a) - y(r,s',d) \\Bigg)^2\n",
    "    \\right]\n",
    "  \\end{equation}\n",
    "\n",
    "    \n",
    "    \n",
    "    where the target is given by \n",
    "   \\begin{equation}\n",
    "    y(r, s', d) = r + \\gamma (1 - d) \\left( \\min_{j=1,2} Q_{\\phi_{\\text{targ},j}}(s', \\tilde{a}') - \\alpha \\log \\pi_{\\theta}(\\tilde{a}'|s') \\right), \\;\\;\\;\\;\\; \\tilde{a}' \\sim \\pi_{\\theta}(\\cdot|s')\n",
    "   \\end{equation}\n",
    "\n",
    "    \n",
    "  - The **Actor network** receives observation as inputs and tries to predict the optimal action, which maximize the expected future return plus expected future entropy\n",
    "  \\begin{split}\n",
    "V^{\\pi}(s)&= E_{a \\sim \\pi}[{Q^{\\pi}(s,a)} + \\alpha H\\left(\\pi(\\cdot|s)\\right)] \\\\\n",
    " &= E_{a \\sim \\pi}[{Q^{\\pi}(s,a) - \\alpha \\log \\pi(a|s)}]\n",
    "\\end{split}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Observervation of graph data - GraphObserver\n",
    "- generate some dummy data to show graph observation\n",
    "- explain a single observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running data_generation on scenario 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "graph() missing 1 required positional argument: 'graph_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7d38f4d4d718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscenario_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_scenarios\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Window popping up due to MPViewer of BARK lib - not our fault!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/report/run.runfiles/bark_ml/supervised_learning/data_generation.py\u001b[0m in \u001b[0;36mrun_scenarios\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mpart\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running data_generation on scenario \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_scenarios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0mdata_scenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m       \u001b[0;31m#time.sleep(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_scenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/report/run.runfiles/bark_ml/supervised_learning/data_generation.py\u001b[0m in \u001b[0;36mrun_scenario\u001b[0;34m(self, scenario)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdata_scenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/report/run.runfiles/bark_ml/bark_ml/environments/single_agent_runtime.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, scenario)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# observe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mobserved_world\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserved_world\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/report/run.runfiles/bark_ml/bark_ml/observers/graph_observer.py\u001b[0m in \u001b[0;36mObserve\u001b[0;34m(self, world)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphObserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: graph() missing 1 required positional argument: 'graph_dims'"
     ]
    }
   ],
   "source": [
    "scenario_data = graph_generator.run_scenarios()\n",
    "# Window popping up due to MPViewer of BARK lib - not our fault!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random data_point from data_set\n",
    "data_point = random.choice(random.choice(scenario_data))\n",
    "observation = data_point[\"graph\"]\n",
    "graph = GraphObserver.graph_from_observation(observation)\n",
    "\n",
    "# Visualize datapoint (graph data)\n",
    "fig = plt.figure(figsize=(4,9))\n",
    "ax  = fig.add_subplot(111)\n",
    "ax.set_xlim(-1,1)\n",
    "ax.set_ylim(-1,1)\n",
    "visualize_graph(data_point, ax, visible_distance, normalization_ref_highway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image from above shows the perceived environment from the agent's perspective in graph structure:\n",
    "- the red node is the $\\color{red}{\\text{controlled/ego agent}}$\n",
    "- the blue agents are $\\color{blue}{\\text{other agents which are not controlled}}$\n",
    "- the yellow zone indicates the <mark>area that agents are visible to the ego agent<mark>. Visibility is further indicated by the missing connections to agents outside the zone\n",
    "- the green point shows the $\\color{green}{\\text{goal of the ego agent}}$\n",
    "\n",
    "About every other agent that the ego agent is perceiving (and about itself) the following information are represented in the individual node features:\n",
    "- Position -> $x$ and $y$-Coordinates\n",
    "- Orientation -> $\\theta$\n",
    "- Velocity -> $v$\n",
    "- Information related to the goal:\n",
    "    - Position of goal -> $x_{goal}$ and $y_{goal}$-Coordinates\n",
    "    - Distance to goal -> $dx$ and $dy$ and $d=\\sqrt {dxÂ²+dyÂ²}$\n",
    "    - Necessary orientation at goal position -> $\\theta_{goal}$\n",
    "    - Necessary velocity at goal position -> $v_{goal}$\n",
    "    \n",
    "The following cell shows the keys under which the node_features are accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.nodes[1].keys()\n",
    "node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into how we apply graph neural networks to our problem, let's have a **very brief** overview about the idea behind them.  \n",
    "Most importantly, they operate on graph structured data, i.e. data consisting of \n",
    "- **Node:** a feature vector (node embedding) of some data entity (and optionally a label), in our case each vehicle is a node\n",
    "- **Edges:** specified links between nodes\n",
    "- **Edge features:** optionally, each link between nodes can have its own feature vector\n",
    "\n",
    "In the section about the `GraphObserver` above, we've already seen how this graph can look like in our scenario. Let's take a step back and use a simplified visualization where the green node represents the ego vehicle and the remaining nodes are other vehicles in its vecinity on the road.\n",
    "\n",
    "![Schematic view of a GNN](images/simple_gnn.png)\n",
    "\n",
    "The ego node is connected to both other nodes (it \"sees\" the other nodes) which in turn do not see each other.\n",
    "\n",
    "Now, the nodes send messages (their current embeddings) along all outgoing links (here, all links are bidrectional), propagated through a neural network. From now on, we refer to this neural network as the _message passing layer(s)_.\n",
    "\n",
    "> **NOTE**  \n",
    "All edges share the same neural network, instead of each edge having its own weights.\n",
    "\n",
    "Each node aggregates all incoming messages using an aggregation function, like summing or averaging. The result is then processed by another neural network, e.g. a recurrent unit, which computes the new embedding of the node.\n",
    "\n",
    "\n",
    "In our project, we have integrated two different libraries that offer GNN implementations:\n",
    "1. [tf2_gnn](https://github.com/microsoft/tf2-gnn): the library that was initially planned to be used in the project\n",
    "2. [Spektral](https://graphneural.network/#installation): a library that supports edge features, which `tf2_gnn` does not\n",
    "\n",
    "In summary, the node embeddings are updated as follows (exemplary):\n",
    "\n",
    "```python\n",
    "embeddings = recurrent_unit(embeddings, sum(edge_mlp(neighbor_embeddings)))\n",
    "```\n",
    "or\n",
    "\n",
    "$$h_{i,t} = \\mathrm{g} \\big(h_{i, t-1}, \\sum^{N_{i, t}}_j \\mathrm{f}(n_{j,t-1}) \\big)$$ where $h_{i,t}$ is the embedding of node $i$ at time $t$, $\\mathrm{g}$ may be a gated recurrent unit at the nodes, $N_{i, t}$ is the number of neighbors (incoming edges) of node $i$ at time $t$, $n_j$ are the neighbors and $\\mathrm{f}$ is a simple feed-forward neural network on the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The `GNNWrapper` class\n",
    "\n",
    "As an abstraction over the specific implementation of the graph neural network, we implemented a wrapper class called `GNNWrapper`. Its primary function is to act just as a GNN and so the only interface is the `call` function that accepts a batch of observations (array representations of graphs) and returns a batch of updated node embeddings for each graph.\n",
    "\n",
    "In order to support `tf2_gnn` and `Soektral`, we have two distinct call implementations, one for each library. The `GNNWrapper` class decides which one to call based on the arguments given in the initialization.\n",
    "\n",
    "Both functions however work almost the same:\n",
    "1. Convert the given observations into nodes, edges and, when using spektral, edges features.\n",
    "2. Call the respective library with the converted graph representation\n",
    "\n",
    "When specifying `Spektral` as the GNN library, the call function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import EdgeConditionedConv, GlobalAttnSumPool\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the layers of the GNN (normally, this happens in the __init__ function)\n",
    "edge_convolution = EdgeConditionedConv(channels=64, kernel_network=[256], activation=\"tanh\")\n",
    "pooling = GlobalAttnSumPool()\n",
    "dense = Dense(units=256, activation=\"tanh\")\n",
    "\n",
    "def call_spektral(self, observations, training=False):\n",
    "    # convert the observations into\n",
    "    # X: tensor containing the node features\n",
    "    # A: binary adjacency matrix specifying edges in the graph\n",
    "    # E: tensor containg edge features\n",
    "    X, A, E = GraphObserver.graph(observations, self._graph_dims)\n",
    "    \n",
    "    # pass the inputs through an edge conditioned convolution\n",
    "    # layer and receive new node embeddings\n",
    "    X = edge_convolution([X, A, E])\n",
    "    \n",
    "    # perform a global pooling operation over the node embeddings\n",
    "    X = pooling(X)\n",
    "    \n",
    "    # pass the noe embeddings through a dense layer\n",
    "    X = dense(X)\n",
    "    \n",
    "    # output the final transformed node embeddings\n",
    "    return X\n",
    "\n",
    "\n",
    "# uncomment the following line to call the function with sample observations\n",
    "#call_spektral(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, when `tf2_gnn` is specified, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf2_gnn.layers import GNN, GNNInput\n",
    "import pprint as pp\n",
    "\n",
    "# the number and types of layers in the GNN are all encoded\n",
    "# in the parameters dictionary\n",
    "gnn_params = GNN.get_default_hyperparameters()\n",
    "\n",
    "# uncomment the following two lines to have a look at them\n",
    "#print(f'GNN parameters:')\n",
    "#pp.pprint(gnn_params)\n",
    "\n",
    "# initialize a GNN instance which acts as a keras layer\n",
    "gnn = GNN(gnn_params)\n",
    "\n",
    "def call_tf2_gnn(self, observations, training=False):\n",
    "    batch_size = tf.constant(observations.shape[0])\n",
    "    \n",
    "    # convert the observations into\n",
    "    # X: tensor containing the node features\n",
    "    # A: dense adjacency list in the format [[0, 1], [2, 4]]\n",
    "    #    specifying source and target node ids of an egde\n",
    "    # node_to_graph_map: a tensor that assigns each node in X to a graph\n",
    "    X, A, node_to_graph_map = GraphObserver.graph(\n",
    "      observations, \n",
    "      graph_dims=self._graph_dims, \n",
    "      dense=True)\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "      node_features=X,\n",
    "      adjacency_lists=(A,),\n",
    "      node_to_graph_map=node_to_graph_map,\n",
    "      num_graphs=batch_size,\n",
    "    )\n",
    "\n",
    "    return self._gnn(gnn_input, training=training)\n",
    "\n",
    "# uncomment the following line to call the function with sample observations\n",
    "#call_tf2_gnn(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the GNN functionality nicely abstracted behind this wrapper, we can now easily integrate it into the Soft-Actor-Critic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Soft-Actor-Critic Algorithm with Graph Neural Networks\n",
    "\n",
    "Next, let's examine the integrated system.\n",
    "\n",
    "Above, we have already discussed how the **Soft-Actor-Critic** algorithm works on a high level and how the SAC-agent is integrated into **BARK-ML**. In our case, we want to exploit the graph-like structure of traffic scenarios and have already encoded the state of the world as a graph. Now, we want to apply graph neural networks to the SAC algorithm. \n",
    "\n",
    "The resulting actor and critic networks are quite similar in structure. Here's how they work and what they compute.\n",
    "\n",
    "### The Actor Network\n",
    "\n",
    "**Input**: a batch of observations of shape _(batch_size, observation_size)_  \n",
    "**Output**: a batch of a normal distributions over the action space from which the policy will sample the actions performed by the agent\n",
    "\n",
    "![Actor Network Architecture](images/actor_architecture.png)\n",
    "\n",
    "**1. GNN**  \n",
    "The observations are directly fed into the graph neural network (a `GNNWrapper` instance). It converts the observations into graphs and computes new node embeddings for each graph by means of message passing and aggregation. Depending on which GNN library is selected, these embeddings are flattened with a either a pooling (`spektral`) or a global exchange (`tf2_gnn`) layer and finally propagated through a dense layer that outputs flattened graph representations.\n",
    "\n",
    "> **NOTE**  \n",
    "From here on, we're only interested in the embeddings of the ego agent. Hence, instead of feeding the whole graph representation into the encoding network, we extract the embeddings of the first node of each graph, which represents the ego agent.\n",
    "\n",
    "**2. Encoding Network**  \n",
    "In the encoding network, the node embeddings of the ego agent are now passed through a series of dense layers. Depending on the parameters passed into the actor, we can also add convolutions, dropout and other types of layers here.\n",
    "\n",
    "**3. Projection Network**  \n",
    "Finally, the projection network receives the hidden representations after the encoding network and computes a normal distribution over the action space for each observation contained in the batch, modeled by a mean and a standard deviation.\n",
    "\n",
    "In a very simplified manner for brevity, the implementation of the actor's `call` function looks as follows:\n",
    "```python\n",
    "def call(self, observations, training=False):\n",
    "    batch_size, feature_len = observations.shape\n",
    "    \n",
    "    # get the updated node embeddings\n",
    "    output = self._gnn(observations, training=training)\n",
    "\n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the ego agent's node embeddings through the encoder\n",
    "    output = self._encoder(output, training=training)\n",
    "    \n",
    "    # compute a normal distribution\n",
    "    output = self._projection_net(output, training=training)\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "### The Critic Network\n",
    "\n",
    "**Input**: a batch of observation-action pairs, i.e. `[obs, action]` with shapes _(batch_size, observation_size)_ and _(batch_size, 2)_  \n",
    "**Output**: a scalar value assigned to each observation-action pair\n",
    "\n",
    "![Critic Network Architecture](images/critic_architecture.png)\n",
    "\n",
    "The major difference compared to the actor network is that in the critic, we have two parallel pipelines for the observations and their corresponding actions.\n",
    "\n",
    "**1. Actions**  \n",
    "The actions are simply passed into an action encoding network that works similar to the encoding network of the actor network, i.e. a series of dense layers with optional convolutions, dropout layers, etc.\n",
    "\n",
    "**2. Observations**  \n",
    "The observations are processed in the exact same way as in the actor network. We compute new graph representations in the GNN, extract the ego node embeddings and pass them through an encoding network.\n",
    "\n",
    "**3. Joining Actions and Observations**  \n",
    "After receiving the outputs from the action and observation encoding networks, we concatenate the observation-action pair of each element in the batch to one feature vector.  \n",
    "Finally, we pass this concatenated state through a fully connected joint network which outputs a scalar value for each observation-action pair.\n",
    "\n",
    "Again, a simplified version of the implemenation looks like this:\n",
    "```python\n",
    "def call(self, inputs, training=False):\n",
    "    observations, actions = inputs\n",
    "    batch_size = observations.shape[0]\n",
    "     \n",
    "    # get the updated node embeddings\n",
    "    node_embeddings = self._gnn.batch_call(observations, training=training)\n",
    "    \n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the node embeddings through their observation encoder\n",
    "    node_embeddings = self._observation_encoder(node_embeddings, trainig=training)\n",
    "    \n",
    "    # do the same for the actions with a different action encoder\n",
    "    actions = self._action_encoder(actions, training=training)\n",
    "    \n",
    "    # concatenate observations and actions into one vector\n",
    "    joint = tf.concat([node_embeddings, actions], 1)\n",
    "    \n",
    "    # compute a scalar output value\n",
    "    output = self._joint_net(joint, training=training)\n",
    "\n",
    "    return output, network_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Evaluation of capabilites of actor net\n",
    "- introduce supervised setting\n",
    "- benchmark GNN-SAC vs SAC, randomActor and ConstantActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apendix: Commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bazel commands from here (no visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!cd /home/silvan/working_bark && bazel run //examples:tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/silvan/working_bark && bazel run //examples:tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
