{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Group 1 in Practical Planning Robust Behavior for autonomous driving\n",
    "# Reinforcement Learning using Graph Neural Networks\n",
    "\n",
    "### Tom DÃ¶rr, Marco Oliva, Quoc Trung Nguyen, Silvan Wimmer\n",
    "\n",
    "__Objective__: Exploit the graph-like structure of traffic scenarios by applying graph neural networks to the Soft-Actor-Critic algorithm.\n",
    "## Chapter 1: Basic Setup\n",
    "### 1.1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docs.report.helper_functions import set_notebook_log_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import pprint as pp\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "# reduce the number of log messages for improved readability\n",
    "import logging\n",
    "import sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "# Bark imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "# Bark-ml imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "from bark_ml.environments.blueprints import ContinuousMergingBlueprint,\\\n",
    "    ContinuousHighwayBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf_agents.agents import BehaviorGraphSACAgent\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "\n",
    "# Supervised tests\n",
    "from bark_ml.tests.capability_gnn_actor.data_handler import SupervisedData\n",
    "from bark_ml.tests.capability_gnn_actor.actor_nets import ConstantActorNet,\\\n",
    "  RandomActorNet\n",
    "\n",
    "# helper_functions\n",
    "from helper_functions import configurable_setup, benchmark_actor, explain_node_attributes,\\\n",
    "    explain_edge_attributes, explain_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDO:\n",
    "- Fuse all imports\n",
    "- Why is GraphObserver.graph() not working?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: GraphObserver\n",
    "In this chapter we want to briefly introduce the working mechanisms of the GraphObserver.\n",
    "\n",
    "<img src=\"images/observer.png\" width=\"700\">\n",
    "\n",
    "\n",
    "The GraphObserver has the following parameters which can be set in with the ParameterServer (i.e. with `params[\"ML\"][\"GraphObserver\"]`):\n",
    "- \"AgentLimit\": The maximum number of agents that can be observed. Default is 4.\n",
    "- \"VisibilityRadius\": The radius in which agent can 'see', i.e. detect other agents. Default is 50.\n",
    "- \"SelfLoops\": Whether each node has an edge pointing to itself. Influences performance i.e. of GNN SAC instances based on spektral.\n",
    "- \"EnabledNodeFeatures\": The list of available node features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_node_attributes`. Not available node features are ignored (but a log message is shown.)\n",
    "- \"EnabledEdgeFeatures\": The list of available edge features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_edge_attributes`. Not available edge features are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available_node_attributes: ['x', 'y', 'theta', 'vel', 'goal_x', 'goal_y', 'goal_dx', 'goal_dy', 'goal_theta', 'goal_d', 'goal_vel']\n",
      "Meaning of node attributes:\n",
      "     'x': x-coordinate in world\n",
      "     'y': y-coordinate in world\n",
      "     'theta': orientation of agent\n",
      "     'vel': velocity of agent in direction of orientation\n",
      "     'goal_x': x-coordinate of goal\n",
      "     'goal_y': y-coordinate of goal\n",
      "     'goal_dx': distance in x-coordinate from agent to goal\n",
      "     'goal_dy': distance in y-coordinate from agent to goal\n",
      "     'goal_theta': difference between goal orientation and agent orientation\n",
      "     'goal_d': distance to goal (straight line)\n",
      "     'goal_vel': velocity, the agent should have when reaching goal\n",
      "\n",
      "Available_edge_attributes: ['dx', 'dy', 'dvel', 'dtheta']\n",
      "Meaning of edge attributes:\n",
      "     'dx': difference in x-coordinate between two agents\n",
      "     'dy': difference in y-coordinate between two agents\n",
      "     'dvel': difference in velocity between two agents\n",
      "     'dtheta': difference in orientation between two agents\n"
     ]
    }
   ],
   "source": [
    "# Show available node_attributes\n",
    "print(\"Available_node_attributes:\", GraphObserver.available_node_attributes())\n",
    "print(\"Meaning of node attributes:\\n\"+explain_node_attributes())\n",
    "\n",
    "# Show available edge_attributes\n",
    "print(\"\\nAvailable_edge_attributes:\", GraphObserver.available_edge_attributes())\n",
    "print(\"Meaning of edge attributes:\\n\"+explain_edge_attributes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Observe(world)` method returns an Observation based on the current snapshot of the world by extracting node attributes, adjacency matrix, and edge attributes per edge. An Observation is a Tensor containing all information of the graph. The Tensor shape is used to be compatible with tf_agents.\n",
    "\n",
    "The `graph(observations, graph_dims, dense=False)` method takes an 'observation' as input (additionally it needs some information about the protcoll of the observation, i.e. where in the tensor what information is stored - the 'graph_dims'). 'dense' is a parameter which specifies the format of the returned graph representation (for further details see the documentation of the method).\n",
    "\n",
    "OK, let's look at a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# create environment\n",
    "params = ParameterServer(filename='data/tfa_gnn_params.json')\n",
    "bp = ContinuousHighwayBlueprint(params, number_of_senarios=2, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)\n",
    "    \n",
    "# create agent\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example explains one exemplary observation and its parts. Further details for the parts can be looked up e.g. in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_attributes(flattened matrix of original shape 4x11) (nodes x node attributes):\n",
      " [ 0.4999322  -0.8763594  -0.5        -0.45761493 -0.4999322   1.\n",
      " -0.4999322   0.9381797  -0.49511522  0.8758025  -0.4        -0.4999322\n",
      " -0.8404766  -0.5        -0.44750327 -0.4999322   1.          0.\n",
      "  0.92023826 -0.5         0.8397137  -0.4         0.4999322  -1.\n",
      " -0.5        -0.40704772 -0.4999322   1.         -0.4999322   1.\n",
      " -0.49541715  0.99937826 -0.4         0.4999322  -0.7510234  -0.5\n",
      " -0.44514397 -0.4999322   1.         -0.4999322   0.8755117  -0.49476564\n",
      "  0.75053436 -0.4       ]\n",
      "Adjacency matrix(flattened matrix of original shape 4x4) (nodes x nodes):\n",
      " [0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      "Edge_attributes(flattened matrix of original shape 16x4) (number of edges x edge attributes):\n",
      " [ 0.          0.          0.          0.          0.9998644  -0.03588281\n",
      " -0.01011166  0.          0.          0.12364063 -0.05056721  0.\n",
      "  0.         -0.12533593 -0.01247097  0.         -0.9998644   0.03588281\n",
      "  0.01011166  0.          0.          0.          0.          0.\n",
      " -0.9998644   0.15952344 -0.04045555  0.         -0.9998644  -0.08945312\n",
      " -0.00235931  0.          0.         -0.12364063  0.05056721  0.\n",
      "  0.9998644  -0.15952344  0.04045555  0.          0.          0.\n",
      "  0.          0.          0.         -0.24897656  0.03809624  0.\n",
      "  0.          0.12533593  0.01247097  0.          0.9998644   0.08945312\n",
      "  0.00235931  0.          0.          0.24897656 -0.03809624  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# let's initialize the environment and call Observe() (this is happening internally in env.reset!)\n",
    "observation = env.reset()\n",
    "\n",
    "explain_observation(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us take a look at the graph() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-78f0a23aa984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/bazel/_bazel_silvan/9feb0a5c10ccf83fd245bd86b828ce9f/execroot/bark_ml/bazel-out/k8-fastbuild/bin/docs/report/run.runfiles/bark_ml/bark_ml/observers/graph_observer.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(cls, observations, graph_dims, dense)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# extract node features F\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_nodes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# extract adjacency matrix A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1148\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10155\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10156\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10157\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10158\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10159\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbegin_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "observer.graph(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Graph Neural Networks\n",
    "Before diving into how we apply graph neural networks to our problem, let's have a **very brief** overview about the idea behind them.  \n",
    "Most importantly, they operate on graph structured data, i.e. data consisting of \n",
    "- **Nodes:** feature vectors (node embeddings) of some data entities (and optionally a label), in our case each vehicle is a node\n",
    "- **Edges:** specified links between nodes\n",
    "- **Edge features:** optionally, each link between nodes can have its own feature vector\n",
    "\n",
    "In the section about the `GraphObserver` above, we've already seen how this graph can look like in our scenario. Let's take a step back and use a simplified visualization where the green node represents the ego vehicle and the remaining nodes are other vehicles in its vicinity on the road.\n",
    "\n",
    "![Schematic view of a GNN](images/simple_gnn.png)\n",
    "\n",
    "The ego node is connected to both other nodes (it \"sees\" the other nodes) which in turn do not see each other.\n",
    "\n",
    "Now, the nodes send messages (their current embeddings) along all outgoing links (here, all links are bidrectional), propagated through a neural network. From now on, we refer to this neural network as the _message passing layer(s)_.\n",
    "\n",
    "> **NOTE**  \n",
    "All edges share the same neural network, instead of each edge having its own weights.\n",
    "\n",
    "Each node aggregates all incoming messages using an aggregation function, like summing or averaging. The result is then processed by another neural network, e.g. a recurrent unit, which computes the new embedding of the node.\n",
    "\n",
    "\n",
    "In our project, we have integrated two different libraries that offer GNN implementations:\n",
    "1. [tf2_gnn](https://github.com/microsoft/tf2-gnn): the library that was initially planned to be used in the project\n",
    "2. [Spektral](https://graphneural.network/#installation): a library that supports edge features, which `tf2_gnn` does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The `GNNWrapper` class\n",
    "\n",
    "As an abstraction over the specific implementation of the graph neural network, we implemented a wrapper class called `GNNWrapper`. Its primary function is to act just as a GNN and so the only interface is the `call` function that accepts a batch of observations (array representations of graphs) and returns a batch of updated node embeddings for each graph.\n",
    "\n",
    "In order to support `tf2_gnn` and `Spektral`, we have two distinct call implementations, one for each library. The `GNNWrapper` class decides which one to call based on the arguments given in the initialization.\n",
    "\n",
    "Both functions however work almost the same:\n",
    "1. Convert the given observations into nodes, edges and, when using `Spektral`, edges features.\n",
    "2. Call the respective library with the converted graph representation.\n",
    "\n",
    "When specifying `Spektral` as the GNN library, the call function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.75324947 0.21065447 0.5193448  0.56075853 0.6621831 ]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.2384678  0.         0.19256851 1.040382   0.18795615 0.\n",
      " 0.87221414 0.         0.         0.26274508]\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import EdgeConditionedConv, GlobalAttnSumPool\n",
    "from tensorflow.keras.layers import Dense\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "from docs.report.helper_functions import get_sample_observations, graph_dims\n",
    "\n",
    "def call_spektral_demo(observations):\n",
    "    # define the layers of the GNN (normally, this happens upon initialization)\n",
    "    \n",
    "    # this defines an edge-conditioned convolution as the message passing layer\n",
    "    # the `kernel_network` argument defines the layers of the edge neural network\n",
    "    edge_convolution = EdgeConditionedConv(channels=16, kernel_network=[128], activation=\"relu\")\n",
    "\n",
    "    def call_spektral(observations, training=False):\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features (embeddings)\n",
    "        # A: binary adjacency matrix specifying edges in the graph\n",
    "        # E: tensor containg edge features\n",
    "        old_embeddings, A, E = GraphObserver.graph(observations, graph_dims)\n",
    "\n",
    "        # pass the inputs through an edge conditioned convolution\n",
    "        # layer and receive new node embeddings\n",
    "        new_embeddings = edge_convolution([old_embeddings, A, E])\n",
    "\n",
    "        # output the final transformed node embeddings\n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_spektral(observations)\n",
    "    \n",
    "    print(\"Here's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_spektral_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, when `tf2_gnn` is specified, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.11097512 0.3341901  0.30166426 0.2869975  0.09270446]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.00809294 0.01876954 0.01764303 0.01134065 0.00685283 0.05259295\n",
      " 0.         0.         0.         0.06448102 0.02972951 0.\n",
      " 0.00982654 0.09037469 0.         0.01321555]\n"
     ]
    }
   ],
   "source": [
    "from tf2_gnn.layers import GNN, GNNInput\n",
    "import tensorflow as tf\n",
    "\n",
    "def call_tf2_gnn_demo(observations):\n",
    "    # the number and types of layers in the GNN are all encoded\n",
    "    # in the parameters dictionary, let's stick to the default for now\n",
    "    gnn_params = GNN.get_default_hyperparameters()\n",
    "\n",
    "    # uncomment the following two lines to have a look at them\n",
    "    # print(f'GNN parameters:')\n",
    "    # pp.pprint(gnn_params)\n",
    "\n",
    "    # initialize a GNN instance which acts as a keras layer\n",
    "    gnn = GNN(gnn_params)\n",
    "\n",
    "    def call_tf2_gnn(observations, training=False):\n",
    "        batch_size = tf.constant(observations.shape[0])\n",
    "\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features\n",
    "        # A: dense adjacency list in the format [[0, 1], [2, 4]]\n",
    "        #    specifying source and target node ids of an egde\n",
    "        # node_to_graph_map: a tensor that assigns each node in X to a graph\n",
    "        old_embeddings, A, node_to_graph_map = GraphObserver.graph(\n",
    "          observations,\n",
    "          graph_dims=graph_dims, \n",
    "          dense=True)\n",
    "        \n",
    "        # build the struct that tf2_gnn expects as input\n",
    "        gnn_input = GNNInput(\n",
    "          node_features=old_embeddings,\n",
    "          adjacency_lists=(A,),\n",
    "          node_to_graph_map=node_to_graph_map,\n",
    "          num_graphs=batch_size,\n",
    "        )\n",
    "\n",
    "        new_embeddings = gnn(gnn_input, training=training)\n",
    "        \n",
    "        # only for demo purposes\n",
    "        old_embeddings = tf.reshape(old_embeddings, [batch_size, graph_dims[0], -1])\n",
    "        new_embeddings = tf.reshape(new_embeddings, [batch_size, 5, -1])\n",
    "        \n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_tf2_gnn(observations)\n",
    "    \n",
    "    print(\"\\nHere's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_tf2_gnn_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the GNN functionality nicely abstracted behind this wrapper, we can now easily integrate it into the Soft-Actor-Critic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Soft-Actor-Critic Algorithm with Graph Neural Networks\n",
    "\n",
    "Next, let's examine the integrated system.\n",
    "\n",
    "We want to exploit the graph-like structure of traffic scenarios and have already encoded the state of the world as a graph. Now, we want to apply graph neural networks to the SAC algorithm. \n",
    "\n",
    "The resulting actor and critic networks are quite similar in structure. Here's how they work and what they compute.\n",
    "\n",
    "### The Actor Network\n",
    "\n",
    "Implemented in the class `GNNActorNetwork`.\n",
    "\n",
    "\n",
    "**Input**: a batch of observations of shape _(batch_size, observation_size)_  \n",
    "**Output**: a batch of a normal distributions over the action space from which the policy will sample the actions performed by the agent\n",
    "\n",
    "![Actor Network Architecture](images/actor_architecture.png)\n",
    "\n",
    "**1. GNN**  \n",
    "The observations are directly fed into the graph neural network (a `GNNWrapper` instance). It converts the observations into graphs and computes new node embeddings for each graph by means of message passing and aggregation. Optionally, the new note embeddings are propagated through a dense layer before being returned.\n",
    "\n",
    "> **NOTE**  \n",
    "From here on, we're only interested in the embeddings of the ego agent. Hence, instead of feeding the whole graph representation into the encoding network, we extract the embeddings of the first node of each graph, which represents the ego agent.\n",
    "\n",
    "**2. Encoding Network**  \n",
    "In the encoding network, the node embeddings of the ego agent are now passed through a series of dense layers. Depending on the parameters passed into the actor, we can also add convolutions, dropout and other types of layers here.\n",
    "\n",
    "**3. Projection Network**  \n",
    "Finally, the projection network receives the hidden representations after the encoding network and computes a normal distribution over the action space for each observation contained in the batch, modeled by a mean and a standard deviation.\n",
    "\n",
    "In a very simplified manner for brevity, the implementation of the actor's `call` function looks as follows:\n",
    "```python\n",
    "def call(self, observations, training=False):\n",
    "    # get the updated node embeddings\n",
    "    output = self._gnn(observations, training=training)\n",
    "\n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the ego agent's node embeddings through the encoder\n",
    "    output = self._encoder(output, training=training)\n",
    "    \n",
    "    # compute a normal distribution\n",
    "    output = self._projection_net(output, training=training)\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "### The Critic Network\n",
    "\n",
    "Implemented in the class `GNNCriticNetwork`.\n",
    "\n",
    "**Input**: a batch of observation-action pairs, i.e. `[obs, action]` with shapes _(batch_size, observation_size)_ and _(batch_size, 2)_  \n",
    "**Output**: a scalar value assigned to each observation-action pair\n",
    "\n",
    "\n",
    "The major difference compared to the actor network is that in the critic, we have two parallel pipelines for the observations and their corresponding actions.\n",
    "\n",
    "![Critic Network Architecture](images/critic_architecture.png)\n",
    "\n",
    "**1. Actions**  \n",
    "The actions are simply passed into an action encoding network that works similar to the encoding network of the actor network, i.e. a series of dense layers with optional convolutions, dropout layers, etc.\n",
    "\n",
    "**2. Observations**  \n",
    "The observations are processed in the exact same way as in the actor network. We compute new graph representations in the GNN, extract the ego node embeddings and pass them through an encoding network.\n",
    "\n",
    "**3. Joining Actions and Observations**  \n",
    "After receiving the outputs from the action and observation encoding networks, we concatenate the observation-action pair of each element in the batch to one feature vector.  \n",
    "Finally, we pass this concatenated state through a fully connected joint network which outputs a scalar value for each observation-action pair.\n",
    "\n",
    "Again, a simplified version of the implemenation looks like this:\n",
    "```python\n",
    "def call(self, inputs, training=False):\n",
    "    observations, actions = inputs\n",
    "     \n",
    "    # get the updated node embeddings\n",
    "    node_embeddings = self._gnn(observations, training=training)\n",
    "    \n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the node embeddings through their observation encoder\n",
    "    node_embeddings = self._observation_encoder(node_embeddings, trainig=training)\n",
    "    \n",
    "    # do the same for the actions with a different action encoder\n",
    "    actions = self._action_encoder(actions, training=training)\n",
    "    \n",
    "    # concatenate observations and actions into one vector\n",
    "    joint = tf.concat([node_embeddings, actions], 1)\n",
    "    \n",
    "    # compute a scalar output value\n",
    "    output = self._joint_net(joint, training=training)\n",
    "\n",
    "    return output, network_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Putting it all together and setting up an example\n",
    "\n",
    "Now, let's set up an SAC-agent using the graph neural networks described above to be used in BARK-ML.\n",
    "\n",
    "We start out with the default parameter set as defined in `tfa_gnn_params.json` and make some optional changes afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterServer(filename='data/tfa_gnn_params.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the GNN-related parameters. We use the same GNN configuration in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a spektral GNN\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['Library'] = 'spektral'\n",
    "    \n",
    "# use two message passing layers with 80 channels of node embeddings each\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"NumMpLayers\"] = 2\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"MpLayersHiddenDim\"] = 80\n",
    "    \n",
    "# use two fully connected layers in the edge feature mlp of each message passing layer\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] = [128, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure the layers that make up the encoding networks in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticJointFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticObservationFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"ActorFcLayerParams\"] = [256, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the `GraphObserver`.\n",
    "Here we specify that it should always observe at most 4 agents simultaneously, i.e. the ego agent and its three nearest agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"GraphObserver\"][\"AgentLimit\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify which features the graph observer should use in the node embeddings and the edges. This is useful since not all environments contain the same information and thus, some features might not be possible to compute. To get a list of all available features, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available node features:\n",
      "'x': The x-components of the agent's position.\n",
      "'y': The y-components of the agent's position.\n",
      "'theta': The current heading angle of tha agent.\n",
      "'vel': The current velocity of the agent.\n",
      "'goal_x': The x-component of the goal's position.\n",
      "'goal_y': The y-component of the goal's position.\n",
      "'goal_dx': The difference in the x-component of the agent's and the goal's position.\n",
      "'goal_dy': The difference in the y-component of the agent's and the goal's position.\n",
      "'goal_theta': The goal heading angle.\n",
      "'goal_d': The euclidian distance of the agent to the goal.\n",
      "'goal_vel': The goal velocity.\n",
      "\n",
      "Available edge features:\n",
      "'dx': The difference in the x-position of the two agents.\n",
      "'dy': The difference in the y-position of the two agents.\n",
      "'dvel': The difference in the velocity of the two agents.\n",
      "'dtheta': The difference in the heading angle of the two agents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available node features:\")\n",
    "for key, value in GraphObserver.available_node_attributes(with_descriptions=True).items():\n",
    "    print(f\"'{key}': {value}\")\n",
    "\n",
    "print(f\"\\nAvailable edge features:\")\n",
    "for key, value in GraphObserver.available_edge_attributes(with_descriptions=True).items():\n",
    "    print(f\"'{key}': {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we train an agent on the `ContinuousMergingBlueprint` where the goal definition does not contain velocity information, so we can not use the `goal_vel` node feature. So let's configure the `GraphObserver` to use all available node features, except `goal_vel`.\n",
    "\n",
    "In the edges, we want to use all available features, so we don't specify anything. The `GraphObserver` always defaults to using all features if nothing is explicitely configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_node_feaures = GraphObserver.available_node_attributes()[:-1]\n",
    "params[\"ML\"][\"GraphObserver\"][\"EnabledNodeFeatures\"] = enabled_node_feaures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you feel like experimenting, expand the following dropdown to get an overview of the most important parameters you can tweak.\n",
    "<details>\n",
    "<summary><b>List of the most important paramaters</b></summary>\n",
    "<br>\n",
    "  <b>Description:</b> Specifies the maximum number of agents that are included in an observation. (int)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['AgentLimit'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies whether each node in the graph will have an edge pointing to itself. (Bool)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['SelfLoops'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the node embeddings. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledNodeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the edges. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledEdgeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the actor encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['ActorFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic action encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticActionFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic observation encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticObservationFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic joint network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticJointFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['NumMpLayers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of units in the message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MpLayersHiddenDim'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies which library to use as the GNN implementation, either \"tf2_gnn\" or \"spektral\". <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['Library'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to TF2-GNN.</h3>\n",
    "\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a relational gated convolution network. (str)\n",
    "      <br><i>NOTE: when using the 'ggnn' message passing layer, 'MpLayersHiddenDim' must match the number of node features!</i> <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.message_calculation_class'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a gated recurrent unit. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['global_exchange_mode'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a dense layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['dense_every_num_layers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a global exchange layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.global_exchange_every_num_layers'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to Spektral.</h3>\n",
    "\n",
    "  <b>Description:</b> Specifies the number of channels in the edge conditioned convolution layer. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPChannels'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) in the edge network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the activation function of the message passing layer. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPLayerActivation'] <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the BARK-ML environment and our agent.  \n",
    "**Pro-tip: run the following cell twice to get a cleaner output with less log messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from docs.report.helper_functions import prepare_agent, summarize_agent\n",
    "        \n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params, number_of_senarios=2500, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)\n",
    "    \n",
    "# create agent\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAGENT SUMMARY\u001b[0m\n",
      "\n",
      "Network                        Parameters\n",
      "==========================================\n",
      "ActorNetwork...................... 547.300\n",
      "CriticNetwork..................... 553.441\n",
      "CriticNetwork2.................... 553.441\n",
      "TargetCriticNetwork1.............. 553.441\n",
      "TargetCriticNetwork2.............. 553.441\n",
      "------------------------------------------\n",
      "Total parameters                 2.761.064\n"
     ]
    }
   ],
   "source": [
    "# only for demo purposes\n",
    "prepare_agent(agent, params, env)\n",
    "summarize_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Evaluate capabilities of actor (with supervised learning)\n",
    "In this part we briefly introduce a supervised learning setting which helps quickly debug different actor implementations. It is evaluated if the actor network is capable of overfitting a small dataset by comparing it to RandomActor and ConstantActor. The RandomActor outputs a random label within a prespecified bound. The ConstantActor outputs the mean label of the training dataset every time. This part basically shows the happenings of the `py_gnn_actor_tests`.\n",
    "\n",
    "Additionally, the performance while learning is compared to the standard SAC actor for better comparison with the help of Tensorboard.\n",
    "\n",
    "This section starts with the definition of some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for default parameter files\n",
    "filename_tf2_gnn = \"../../examples/example_params/tfa_sac_gnn_spektral_default.json\"\n",
    "filename_spektral = \"../../examples/example_params/tfa_sac_gnn_tf2_gnn_default.json\"\n",
    "filename_normal = \"../../examples/example_params/tfa_params.json\"\n",
    "\n",
    "\n",
    "params_tf2_gnn = ParameterServer(filename=filename_tf2_gnn)\n",
    "params_spektral = ParameterServer(filename=filename_spektral)\n",
    "params_normal = ParameterServer(filename=filename_normal)\n",
    "\n",
    "# Some more parameter\n",
    "num_scenarios = 3\n",
    "log_dir = \"supervised/summary\"\n",
    "data_path = \"supervised/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the different actors for benchmarking and fetch or load a small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Get observer and tf2_gnn_actor\n",
    "_, tf2_gnn_actor = configurable_setup(params_tf2_gnn, num_scenarios=num_scenarios);\n",
    "# Get actor based on spektral\n",
    "observer, spektral_actor = configurable_setup(params_spektral, num_scenarios=num_scenarios);\n",
    "# Get normal SAC actor\n",
    "_, normal_sac_actor = configurable_setup(params_normal, num_scenarios=num_scenarios, graph_sac=False);\n",
    "\n",
    "# construct dataset\n",
    "dataset = SupervisedData(observer, params_tf2_gnn, batch_size=32, train_split=0.8,\n",
    "                         data_path=data_path, num_scenarios=num_scenarios);\n",
    "\n",
    "# Get constant actor (outputs constant mean labels of train_dataset)\n",
    "constant_actor = ConstantActorNet(dataset=dataset._train_dataset)\n",
    "# Get random actor (outputs random labels with uniform distribution within bounds)\n",
    "random_actor = RandomActorNet(low=[0, -0.4], high=[0.1, 0.4])\n",
    "\n",
    "actors = {\"tf2_gnn_actor\":{\"actor\":tf2_gnn_actor},\n",
    "          \"spektral_actor\":{\"actor\":spektral_actor},\n",
    "          \"normal_sac_actor\":{\"actor\":normal_sac_actor},\n",
    "          \"random_actor\":{\"actor\":random_actor},\n",
    "          \"consant_actor\":{\"actor\":constant_actor}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the magic starts:\n",
    "Every actor is trained (the RandomActor and ConstantActor are not trainable - see their definition from above) for some epochs. The results of the training can be examined in the tensorboard below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silvan/working_bark/bark_ml/python_wrapper/venv/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Delete all old logs if some exist\n",
    "if os.path.exists(log_dir):\n",
    "    [shutil.rmtree(os.path.join(log_dir,log)) for log in os.listdir(log_dir)];\n",
    "    old_logs = os.listdir(log_dir)\n",
    "else:\n",
    "    old_logs = list()\n",
    "    \n",
    "# Run benchmarking\n",
    "for actor_name in actors:\n",
    "    time.sleep(1)\n",
    "    loss = benchmark_actor(actors[actor_name][\"actor\"], dataset, epochs=100, log_dir=log_dir)\n",
    "    actors[actor_name][\"loss\"] = loss\n",
    "    \n",
    "    # Name log clearly with actor_name\n",
    "    # Select correct log\n",
    "    new_logs = os.listdir(log_dir)\n",
    "    log = list(set(new_logs) - set(old_logs))[0]\n",
    "    # Rename log with actor_name\n",
    "    old_path = os.path.join(log_dir, log)\n",
    "    new_path = os.path.join(log_dir, actor_name)\n",
    "    os.rename(old_path, new_path)\n",
    "    old_logs = os.listdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17109), started 2:17:45 ago. (Use '!kill 17109' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3e695bf258f1dc5b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3e695bf258f1dc5b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir supervised/summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Result and Evaluation\n",
    "- introduce supervised setting\n",
    "- benchmark GNN-SAC vs SAC, randomActor and ConstantActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix: Commands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start a training run from the command line with `bazel run //examples:tfa_gnn -- --mode=train`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
