{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Group 1 in Practical Planning Robust Behavior for autonomous driving\n",
    "# Reinforcement Learning using Graph Neural Networks\n",
    "\n",
    "### Tom DÃ¶rr, Marco Oliva, Quoc Trung Nguyen, Silvan Wimmer\n",
    "\n",
    "__Objective__: Exploit the graph-like structure of tragic scenarios by applying graph neural networks to the Soft-Actor-Critic algorithm.\n",
    "## Chapter 1: Basic Setup\n",
    "### 1.1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "from bark_ml.environments.blueprints import ContinuousHighwayBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf_agents.agents import BehaviorGraphSACAgent\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into how we apply graph neural networks to our problem, let's have a **very brief** overview about the idea behind them.  \n",
    "Most importantly, they operate on graph structured data, i.e. data consisting of \n",
    "- **Nodes:** feature vectors (node embeddings) of some data entities (and optionally a label), in our case each vehicle is a node\n",
    "- **Edges:** specified links between nodes\n",
    "- **Edge features:** optionally, each link between nodes can have its own feature vector\n",
    "\n",
    "In the section about the `GraphObserver` above, we've already seen how this graph can look like in our scenario. Let's take a step back and use a simplified visualization where the green node represents the ego vehicle and the remaining nodes are other vehicles in its vicinity on the road.\n",
    "\n",
    "![Schematic view of a GNN](images/simple_gnn.png)\n",
    "\n",
    "The ego node is connected to both other nodes (it \"sees\" the other nodes) which in turn do not see each other.\n",
    "\n",
    "Now, the nodes send messages (their current embeddings) along all outgoing links (here, all links are bidrectional), propagated through a neural network. From now on, we refer to this neural network as the _message passing layer(s)_.\n",
    "\n",
    "> **NOTE**  \n",
    "All edges share the same neural network, instead of each edge having its own weights.\n",
    "\n",
    "Each node aggregates all incoming messages using an aggregation function, like summing or averaging. The result is then processed by another neural network, e.g. a recurrent unit, which computes the new embedding of the node.\n",
    "\n",
    "\n",
    "In our project, we have integrated two different libraries that offer GNN implementations:\n",
    "1. [tf2_gnn](https://github.com/microsoft/tf2-gnn): the library that was initially planned to be used in the project\n",
    "2. [Spektral](https://graphneural.network/#installation): a library that supports edge features, which `tf2_gnn` does not\n",
    "\n",
    "In summary, the node embeddings are updated as follows (exemplary):\n",
    "\n",
    "```python\n",
    "embeddings = recurrent_unit(embeddings, sum(edge_mlp(neighbor_embeddings)))\n",
    "```\n",
    "or\n",
    "\n",
    "$$h_{i,t} = \\mathrm{g} \\big(h_{i, t-1}, \\sum^{N_{i, t}}_j \\mathrm{f}(n_{j,t-1}) \\big)$$ where $h_{i,t}$ is the embedding of node $i$ at time $t$, $\\mathrm{g}$ may be a gated recurrent unit at the nodes, $N_{i, t}$ is the number of neighbors (incoming edges) of node $i$ at time $t$, $n_j$ are the neighbors and $\\mathrm{f}$ is a simple feed-forward neural network on the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The `GNNWrapper` class\n",
    "\n",
    "As an abstraction over the specific implementation of the graph neural network, we implemented a wrapper class called `GNNWrapper`. Its primary function is to act just as a GNN and so the only interface is the `call` function that accepts a batch of observations (array representations of graphs) and returns a batch of updated node embeddings for each graph.\n",
    "\n",
    "In order to support `tf2_gnn` and `Spektral`, we have two distinct call implementations, one for each library. The `GNNWrapper` class decides which one to call based on the arguments given in the initialization.\n",
    "\n",
    "Both functions however work almost the same:\n",
    "1. Convert the given observations into nodes, edges and, when using `Spektral`, edges features.\n",
    "2. Call the respective library with the converted graph representation.\n",
    "\n",
    "When specifying `Spektral` as the GNN library, the call function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.02944617 0.531453   0.9274401  0.39086574 0.07298207]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.42647105 0.30279604 0.         0.29276448 0.2175161  0.09178288\n",
      " 0.         0.39766562 0.         0.         0.42999676 0.15180485\n",
      " 0.02260317 0.         0.68303376 0.45430782]\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import EdgeConditionedConv, GlobalAttnSumPool\n",
    "from tensorflow.keras.layers import Dense\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "from docs.report.helper_functions import get_sample_observations, graph_dims\n",
    "\n",
    "def call_spektral_demo(observations):\n",
    "    # define the layers of the GNN (normally, this happens upon initialization)\n",
    "    \n",
    "    # this defines an edge-conditioned convolution as the message passing layer\n",
    "    # the `kernel_network` argument defines the layers of the edge neural network\n",
    "    edge_convolution = EdgeConditionedConv(channels=16, kernel_network=[128], activation=\"relu\")\n",
    "    \n",
    "    # after message passing, we'll to pass the node embeddings though a dense layer\n",
    "    dense = Dense(units=256, activation=\"relu\")\n",
    "\n",
    "    def call_spektral(observations, training=False):\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features (embeddings)\n",
    "        # A: binary adjacency matrix specifying edges in the graph\n",
    "        # E: tensor containg edge features\n",
    "        old_embeddings, A, E = GraphObserver.graph(observations, graph_dims)\n",
    "\n",
    "        # pass the inputs through an edge conditioned convolution\n",
    "        # layer and receive new node embeddings\n",
    "        new_embeddings = edge_convolution([old_embeddings, A, E])\n",
    "        \n",
    "        # pass the new node embeddings through a dense layer\n",
    "        X = dense(new_embeddings)\n",
    "\n",
    "        # output the final transformed node embeddings\n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_spektral(observations)\n",
    "    \n",
    "    print(\"Here's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# uncomment the following line to call the function with sample observations\n",
    "call_spektral_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, when `tf2_gnn` is specified, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf2_gnn.layers import GNN, GNNInput\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "\n",
    "def call_tf2_gnn_demo(observations):\n",
    "    # the number and types of layers in the GNN are all encoded\n",
    "    # in the parameters dictionary, let's stick to the default for now\n",
    "    gnn_params = GNN.get_default_hyperparameters()\n",
    "\n",
    "    # uncomment the following two lines to have a look at them\n",
    "    print(f'GNN parameters:')\n",
    "    pp.pprint(gnn_params)\n",
    "\n",
    "    # initialize a GNN instance which acts as a keras layer\n",
    "    gnn = GNN(gnn_params)\n",
    "\n",
    "    def call_tf2_gnn(observations, training=False):\n",
    "        batch_size = tf.constant(observations.shape[0])\n",
    "\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features\n",
    "        # A: dense adjacency list in the format [[0, 1], [2, 4]]\n",
    "        #    specifying source and target node ids of an egde\n",
    "        # node_to_graph_map: a tensor that assigns each node in X to a graph\n",
    "        old_embeddings, A, node_to_graph_map = GraphObserver.graph(\n",
    "          observations,\n",
    "          graph_dims=graph_dims, \n",
    "          dense=True)\n",
    "        \n",
    "        # build the struct that tf2_gnn expects as input\n",
    "        gnn_input = GNNInput(\n",
    "          node_features=old_embeddings,\n",
    "          adjacency_lists=(A,),\n",
    "          node_to_graph_map=node_to_graph_map,\n",
    "          num_graphs=batch_size,\n",
    "        )\n",
    "\n",
    "        new_embeddings = gnn(gnn_input, training=training)\n",
    "        \n",
    "        # only for demo purposes\n",
    "        old_embeddings = tf.reshape(old_embeddings, [batch_size, graph_dims[0], -1])\n",
    "        new_embeddings = tf.reshape(new_embeddings, [batch_size, 5, -1])\n",
    "        \n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_tf2_gnn(observations)\n",
    "    \n",
    "    print(\"\\nHere's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# uncomment the following line to call the function with sample observations\n",
    "#call_tf2_gnn_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the GNN functionality nicely abstracted behind this wrapper, we can now easily integrate it into the Soft-Actor-Critic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Soft-Actor-Critic Algorithm with Graph Neural Networks\n",
    "\n",
    "Next, let's examine the integrated system.\n",
    "\n",
    "We want to exploit the graph-like structure of traffic scenarios and have already encoded the state of the world as a graph. Now, we want to apply graph neural networks to the SAC algorithm. \n",
    "\n",
    "The resulting actor and critic networks are quite similar in structure. Here's how they work and what they compute.\n",
    "\n",
    "### The Actor Network\n",
    "\n",
    "Implemented in the class `GNNActorNetwork`.\n",
    "\n",
    "\n",
    "**Input**: a batch of observations of shape _(batch_size, observation_size)_  \n",
    "**Output**: a batch of a normal distributions over the action space from which the policy will sample the actions performed by the agent\n",
    "\n",
    "![Actor Network Architecture](images/actor_architecture.png)\n",
    "\n",
    "**1. GNN**  \n",
    "The observations are directly fed into the graph neural network (a `GNNWrapper` instance). It converts the observations into graphs and computes new node embeddings for each graph by means of message passing and aggregation. Optionally, the new note embeddings are propagated through a dense layer before being returned.\n",
    "\n",
    "> **NOTE**  \n",
    "From here on, we're only interested in the embeddings of the ego agent. Hence, instead of feeding the whole graph representation into the encoding network, we extract the embeddings of the first node of each graph, which represents the ego agent.\n",
    "\n",
    "**2. Encoding Network**  \n",
    "In the encoding network, the node embeddings of the ego agent are now passed through a series of dense layers. Depending on the parameters passed into the actor, we can also add convolutions, dropout and other types of layers here.\n",
    "\n",
    "**3. Projection Network**  \n",
    "Finally, the projection network receives the hidden representations after the encoding network and computes a normal distribution over the action space for each observation contained in the batch, modeled by a mean and a standard deviation.\n",
    "\n",
    "In a very simplified manner for brevity, the implementation of the actor's `call` function looks as follows:\n",
    "```python\n",
    "def call(self, observations, training=False):\n",
    "    # get the updated node embeddings\n",
    "    output = self._gnn(observations, training=training)\n",
    "\n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the ego agent's node embeddings through the encoder\n",
    "    output = self._encoder(output, training=training)\n",
    "    \n",
    "    # compute a normal distribution\n",
    "    output = self._projection_net(output, training=training)\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "### The Critic Network\n",
    "\n",
    "Implemented in the class `GNNCriticNetwork`.\n",
    "\n",
    "**Input**: a batch of observation-action pairs, i.e. `[obs, action]` with shapes _(batch_size, observation_size)_ and _(batch_size, 2)_  \n",
    "**Output**: a scalar value assigned to each observation-action pair\n",
    "\n",
    "\n",
    "The major difference compared to the actor network is that in the critic, we have two parallel pipelines for the observations and their corresponding actions.\n",
    "\n",
    "![Critic Network Architecture](images/critic_architecture.png)\n",
    "\n",
    "**1. Actions**  \n",
    "The actions are simply passed into an action encoding network that works similar to the encoding network of the actor network, i.e. a series of dense layers with optional convolutions, dropout layers, etc.\n",
    "\n",
    "**2. Observations**  \n",
    "The observations are processed in the exact same way as in the actor network. We compute new graph representations in the GNN, extract the ego node embeddings and pass them through an encoding network.\n",
    "\n",
    "**3. Joining Actions and Observations**  \n",
    "After receiving the outputs from the action and observation encoding networks, we concatenate the observation-action pair of each element in the batch to one feature vector.  \n",
    "Finally, we pass this concatenated state through a fully connected joint network which outputs a scalar value for each observation-action pair.\n",
    "\n",
    "Again, a simplified version of the implemenation looks like this:\n",
    "```python\n",
    "def call(self, inputs, training=False):\n",
    "    observations, actions = inputs\n",
    "     \n",
    "    # get the updated node embeddings\n",
    "    node_embeddings = self._gnn(observations, training=training)\n",
    "    \n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the node embeddings through their observation encoder\n",
    "    node_embeddings = self._observation_encoder(node_embeddings, trainig=training)\n",
    "    \n",
    "    # do the same for the actions with a different action encoder\n",
    "    actions = self._action_encoder(actions, training=training)\n",
    "    \n",
    "    # concatenate observations and actions into one vector\n",
    "    joint = tf.concat([node_embeddings, actions], 1)\n",
    "    \n",
    "    # compute a scalar output value\n",
    "    output = self._joint_net(joint, training=training)\n",
    "\n",
    "    return output, network_state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "The following cell sets up an SAC-agent using the graph neural networks described above to be used in BARK-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CriticNetwork_GNN with 'spektral'...\n",
      "Initializing ActorNetwork_GNN with 'spektral'...\n",
      "[256, 128]\n",
      "Initializing TargetCriticNetwork1_GNN with 'spektral'...\n",
      "Initializing CriticNetwork2_GNN with 'spektral'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No checkpoint available at \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TargetCriticNetwork2_GNN with 'spektral'...\n",
      "\n",
      "Network                        Parameters\n",
      "==========================================\n",
      "ActorNetwork...................... 298.820\n",
      "CriticNetwork..................... 265.921\n",
      "CriticNetwork2.................... 265.921\n",
      "TargetCriticNetwork1.............. 265.921\n",
      "TargetCriticNetwork2.............. 265.921\n",
      "------------------------------------------\n",
      "Total parameters                 1.362.504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from docs.report.helper_functions import prepare_agent, summarize_agent\n",
    "\n",
    "def init_agent():\n",
    "    # set up parameters\n",
    "    params = ParameterServer(filename='data/tfa_params.json')\n",
    "    params['World']['remove_agents_out_of_map'] = False\n",
    "    params['ML']['BehaviorGraphSACAgent']['ActorFcLayerParams'] = [256, 128]\n",
    "    params['ML']['BehaviorGraphSACAgent']['GNN']['library'] = 'spektral'\n",
    "    params['ML']['BehaviorGraphSACAgent']['GNN']['GraphDimensions'] = [4, 11, 4]\n",
    "    \n",
    "    # create environment\n",
    "    bp = ContinuousHighwayBlueprint(params, number_of_senarios=2500, random_seed=0)\n",
    "    observer = GraphObserver(params=params)\n",
    "    env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)\n",
    "    \n",
    "    # create agent\n",
    "    agent = BehaviorGraphSACAgent(environment=env, params=params)\n",
    "    prepare_agent(agent, params, env)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "agent = init_agent()\n",
    "summarize_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you feel like experimenting with the agent, execute the following cell to get an overview of the most important parameters you can tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPath:\u001b[0m [\"ML\"][\"GraphObserver\"][\"AgentLimit\"] (int)\n",
      "\u001b[1mDescription:\u001b[0m Specifies the maximum number of agents that are included in an observation.\n",
      "\u001b[1mDefault value:\u001b[0m 4\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['CriticJointFcLayerParams']\n",
      "\u001b[1mDescription:\u001b[0m specifies the fully connected layers (number and sizes) of the critic joint network ([int])\n",
      "\u001b[1mDefault value:\u001b[0m [256, 128]\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['ActorFcLayerParams']\n",
      "\u001b[1mDescription:\u001b[0m specifies the fully connected layers (number and sizes) of the actor encoding network ([int])\n",
      "\u001b[1mDefault value:\u001b[0m [256, 128]\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['NumMpLayers']\n",
      "\u001b[1mDescription:\u001b[0m specifies the number of message passing layers in the GNN (int)\n",
      "\u001b[1mDefault value:\u001b[0m 2\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['MpLayerNumUnits']\n",
      "\u001b[1mDescription:\u001b[0m the number of units in the message passing layers in the GNN (int)\n",
      "\u001b[1mDefault value:\u001b[0m 128\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['library']\n",
      "\u001b[1mDescription:\u001b[0m which library to use as the GNN implementation, either \"tf2_gnn\" or \"spektral\"\n",
      "\u001b[1mDefault value:\u001b[0m tf2_gnn\n",
      "\n",
      "The following paramater only apply when using tf2_gnn.\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN.message_calculation_class']\n",
      "\u001b[1mDescription:\u001b[0m the identifier of the message passing class to be used, here: a recurrent gated convolution network (str)\n",
      "# NOTE: when using the 'ggnn' message passing layer, 'MPLayerUnits' must match n_features!\n",
      "\u001b[1mDefault value:\u001b[0m rgcn\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['global_exchange_mode']\n",
      "\u001b[1mDescription:\u001b[0m the identifier of the message passing class to be used, here: a gated recurrent unit (str)\n",
      "\u001b[1mDefault value:\u001b[0m gru\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['dense_every_num_layers']\n",
      "\u001b[1mDescription:\u001b[0m specifies after how many message passing layers a dense layer is inserted (int)\n",
      "\u001b[1mDefault value:\u001b[0m 2\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN.global_exchange_every_num_layers']\n",
      "\u001b[1mDescription:\u001b[0m specifies after how many message passing layers a global exchange layer is inserted (int)\n",
      "\u001b[1mDefault value:\u001b[0m 2\n",
      "\n",
      "The following paramater only apply when using Spektral.\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['MPChannels']\n",
      "\u001b[1mDescription:\u001b[0m the number of channels in the edge conditioned convolution layer (int)\n",
      "\u001b[1mDefault value:\u001b[0m 64\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['KernelNetUnits']\n",
      "\u001b[1mDescription:\u001b[0m specifies the fully connected layers (number and sizes) in the edge network ([int])\n",
      "\u001b[1mDefault value:\u001b[0m [256]\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN']['MPLayerActivation']\n",
      "\u001b[1mDescription:\u001b[0m the activation function of the message passing layer (str)\n",
      "\u001b[1mDefault value:\u001b[0m relu\n",
      "\n",
      "\u001b[1mPath:\u001b[0m ['ML']['BehaviorGraphSACAgent']['GNN.DenseActication']\n",
      "\u001b[1mDescription:\u001b[0m the activation function of the final dense layer (str)\n",
      "\u001b[1mDefault value:\u001b[0m tanh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docs.report.helper_functions import print_parameter_overview\n",
    "print_parameter_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Result and Evaluation\n",
    "- introduce supervised setting\n",
    "- benchmark GNN-SAC vs SAC, randomActor and ConstantActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apendix: Commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bazel commands from here (no visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!cd /home/silvan/working_bark && bazel run //examples:tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/silvan/working_bark && bazel run //examples:tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
