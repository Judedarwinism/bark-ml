{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Group 1 in Practical Planning Robust Behavior for Autonomous Driving\n",
    "# Reinforcement Learning using Graph Neural Networks\n",
    "\n",
    "#### Tom Dörr, Marco Oliva, Quoc Trung Nguyen, Silvan Wimmer (Mentor: Patrick Hart)\n",
    "\n",
    "__Objective__: Exploit the graph-like structure of traffic scenarios by applying graph neural networks to the Soft-Actor-Critic algorithm.\n",
    "\n",
    "> ⚠️ **NOTE**  \n",
    "To run this notebook with all dependencies, execute the command `bazel run //docs/gnn_practial_course_2020:run`\n",
    "\n",
    "## Chapter 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import pprint as pp\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "# BARK imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "# BARK-ML imports\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "from bark_ml.environments.blueprints import ContinuousMergingBlueprint, ContinuousHighwayBlueprint\n",
    "from bark_ml.environments.single_agent_runtime import SingleAgentRuntime\n",
    "from bark_ml.library_wrappers.lib_tf_agents.agents import BehaviorGraphSACAgent\n",
    "from bark_ml.observers.graph_observer import GraphObserver\n",
    "\n",
    "# Supervised tests\n",
    "from bark_ml.tests.capability_gnn_actor.data_handler import SupervisedData\n",
    "from bark_ml.tests.capability_gnn_actor.actor_nets import ConstantActorNet, RandomActorNet\n",
    "\n",
    "# Report helper functions\n",
    "from helper_functions import configurable_setup, benchmark_actor, explain_observation, clean_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: The GraphObserver\n",
    "To convert the state of the world into a graph, we needed to implement a new observer.\n",
    "In this chapter, we briefly introduce the working mechanisms of the `GraphObserver`.\n",
    "\n",
    "<img src=\"images/observer.png\" width=\"700\">\n",
    "\n",
    "\n",
    "The `GraphObserver` has the following parameters which can be set in with the ParameterServer (i.e. with `params[\"ML\"][\"GraphObserver\"]`):\n",
    "- **AgentLimit**: The maximum number of agents that can be observed. Default is `4`.\n",
    "- **NormalizationEnabled**: Whether node and edge features are normalized.\n",
    "- **VisibilityRadius**: The radius in which an agent can 'see', i.e. detect other agents. Default is `50`.\n",
    "- **SelfLoops**: Whether each node has an edge pointing to itself. Default is `False`\n",
    "- **EnabledNodeFeatures**: The list of available node features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_node_attributes()`.\n",
    "- **EnabledEdgeFeatures**: The list of available edge features, given by their string key that the observer should extract from the world and insert into the observation. For a list of available features, refer to the list returned by `GraphObserver.available_edge_attributes()`.\n",
    "\n",
    "There are two main interfaces:\n",
    "\n",
    "- The `Observe(world)` method returns an observation based on the current snapshot of the world by extracting node attributes, an adjacency matrix, and edge attributes. An observation is a 1D `tf.Tensor` concatenating all information of the graph.\n",
    "\n",
    "- The `graph(observations, graph_dims, dense=False)` method basically performs the inverse operation. It accepts a batch of observations (as generated by the `Observe` method) as input and returns a batch of graphs. The argument `dense` specifies the format of the returned graph representation (for further details see the function docstring).\n",
    "\n",
    "Let's look at a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# create an environment that we want to observe\n",
    "params = ParameterServer(filename='data/tfa_gnn_params.json')\n",
    "bp = ContinuousHighwayBlueprint(params, number_of_senarios=2, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example explains one exemplary observation and its parts. Further details for the parts can be looked up e.g. in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_attributes(flattened matrix of original shape 4x11) (nodes x node attributes):\n",
      " [ 0.4999322  -0.74460936 -0.5        -0.48046294 -0.4999322   1.\n",
      " -0.4999322   0.8723047  -0.4947464   0.7441238  -0.4        -0.4999322\n",
      " -0.7673594  -0.5        -0.4300491  -0.4999322   1.          0.\n",
      "  0.8836797  -0.5         0.76662683 -0.4        -0.4999322  -0.68209374\n",
      " -0.5        -0.42757058 -0.4999322   1.          0.          0.84104687\n",
      " -0.5         0.68139654 -0.4         0.4999322  -0.6506484  -0.5\n",
      " -0.42485362 -0.4999322   1.         -0.4999322   0.82532424 -0.49444738\n",
      "  0.6502153  -0.4       ]\n",
      "Adjacency matrix(flattened matrix of original shape 4x4) (nodes x nodes):\n",
      " [0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      "Edge_attributes(flattened matrix of original shape 16x4) (number of edges x edge attributes):\n",
      " [ 0.          0.          0.          0.          0.9998644   0.02275\n",
      " -0.05041386  0.          0.9998644  -0.06251562 -0.05289238  0.\n",
      "  0.         -0.09396094 -0.05560932  0.         -0.9998644  -0.02275\n",
      "  0.05041386  0.          0.          0.          0.          0.\n",
      "  0.         -0.08526562 -0.00247852  0.         -0.9998644  -0.11671094\n",
      " -0.00519546  0.         -0.9998644   0.06251562  0.05289238  0.\n",
      "  0.          0.08526562  0.00247852  0.          0.          0.\n",
      "  0.          0.         -0.9998644  -0.03144531 -0.00271694  0.\n",
      "  0.          0.09396094  0.05560932  0.          0.9998644   0.11671094\n",
      "  0.00519546  0.          0.9998644   0.03144531  0.00271694  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# let's initialize the environment and call Observe() (this is happening internally in env.reset)\n",
    "observation = env.reset()\n",
    "\n",
    "explain_observation(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check out what happens when we convert this observation back into a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4, 11), dtype=float32, numpy=\n",
       " array([[[ 0.4999322 , -0.74460936, -0.5       , -0.48046294,\n",
       "          -0.4999322 ,  1.        , -0.4999322 ,  0.8723047 ,\n",
       "          -0.4947464 ,  0.7441238 , -0.4       ],\n",
       "         [-0.4999322 , -0.7673594 , -0.5       , -0.4300491 ,\n",
       "          -0.4999322 ,  1.        ,  0.        ,  0.8836797 ,\n",
       "          -0.5       ,  0.76662683, -0.4       ],\n",
       "         [-0.4999322 , -0.68209374, -0.5       , -0.42757058,\n",
       "          -0.4999322 ,  1.        ,  0.        ,  0.84104687,\n",
       "          -0.5       ,  0.68139654, -0.4       ],\n",
       "         [ 0.4999322 , -0.6506484 , -0.5       , -0.42485362,\n",
       "          -0.4999322 ,  1.        , -0.4999322 ,  0.82532424,\n",
       "          -0.49444738,  0.6502153 , -0.4       ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=\n",
       " array([[[0., 1., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4, 4, 4), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.9998644 ,  0.02275   , -0.05041386,  0.        ],\n",
       "          [ 0.9998644 , -0.06251562, -0.05289238,  0.        ],\n",
       "          [ 0.        , -0.09396094, -0.05560932,  0.        ]],\n",
       " \n",
       "         [[-0.9998644 , -0.02275   ,  0.05041386,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 0.        , -0.08526562, -0.00247852,  0.        ],\n",
       "          [-0.9998644 , -0.11671094, -0.00519546,  0.        ]],\n",
       " \n",
       "         [[-0.9998644 ,  0.06251562,  0.05289238,  0.        ],\n",
       "          [ 0.        ,  0.08526562,  0.00247852,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [-0.9998644 , -0.03144531, -0.00271694,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.09396094,  0.05560932,  0.        ],\n",
       "          [ 0.9998644 ,  0.11671094,  0.00519546,  0.        ],\n",
       "          [ 0.9998644 ,  0.03144531,  0.00271694,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ]]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = tf.expand_dims(observation, 0) # add a batch dimension\n",
    "observer.graph(observation, observer.graph_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Graph Neural Networks\n",
    "Before diving into how we apply graph neural networks to our problem, let's have a **very brief** overview about the idea behind them.  \n",
    "Most importantly, they operate on graph structured data, i.e. data consisting of \n",
    "- **Nodes:** feature vectors (node embeddings) of some data entities (and optionally a label), in our case each vehicle is a node\n",
    "- **Edges:** specified links between nodes\n",
    "- **Edge features:** optionally, each link between nodes can have its own feature vector\n",
    "\n",
    "In the section about the `GraphObserver` above, we've already seen how this graph can look like in our scenario. Let's take a step back and use a simplified visualization where the green node represents the ego vehicle and the remaining nodes are other vehicles in its vicinity on the road.\n",
    "\n",
    "![Schematic view of a GNN](images/simple_gnn.png)\n",
    "\n",
    "The ego node is connected to both other nodes (it \"sees\" the other nodes) which in turn do not see each other.\n",
    "\n",
    "Now, the nodes send messages (their current embeddings) along all outgoing links (here, all links are bidrectional), propagated through a neural network. From now on, we refer to this neural network as the _message passing layer(s)_.\n",
    "\n",
    "> **NOTE**  \n",
    "All edges share the same neural network, instead of each edge having its own weights.\n",
    "\n",
    "Each node aggregates all incoming messages using an aggregation function, like summing or averaging. The result is then processed by another neural network, e.g. a recurrent unit, which computes the new embedding of the node.\n",
    "\n",
    "\n",
    "In our project, we have integrated two different libraries that offer GNN implementations:\n",
    "1. [tf2_gnn](https://github.com/microsoft/tf2-gnn): the library that was initially planned to be used in the project\n",
    "2. [Spektral](https://graphneural.network/#installation): a library that supports edge features, which `tf2_gnn` does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: The `GNNWrapper` class\n",
    "\n",
    "As an abstraction over the specific implementation of the graph neural network, we implemented a wrapper class called `GNNWrapper`. Its primary function is to act just as a GNN and so the only interface is the `call` function that accepts a batch of observations (array representations of graphs) and returns a batch of updated node embeddings for each graph.\n",
    "\n",
    "In order to support `tf2_gnn` and `Spektral`, we have two distinct call implementations, one for each library. The `GNNWrapper` class decides which one to call based on the arguments given in the initialization.\n",
    "\n",
    "Both functions however work almost the same:\n",
    "1. Convert the given observations into nodes, edges and, when using `Spektral`, edges features.\n",
    "2. Call the respective library with the converted graph representation.\n",
    "\n",
    "When specifying `Spektral` as the GNN library, the call function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.8118744  0.710344   0.06502968 0.3774874  0.6960453 ]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[1.063333   0.         0.         0.18281767 0.         0.\n",
      " 0.03388178 0.         0.718655   0.         0.         0.\n",
      " 0.00926232 0.98597205 0.         0.36655983]\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import EdgeConditionedConv\n",
    "from helper_functions import get_sample_observations, graph_dims\n",
    "\n",
    "def call_spektral_demo(observations):\n",
    "    # define the layers of the GNN (normally, this happens upon initialization)\n",
    "    \n",
    "    # this defines an edge-conditioned convolution as the message passing layer\n",
    "    # the `kernel_network` argument defines the layers of the edge neural network\n",
    "    edge_convolution = EdgeConditionedConv(channels=16, kernel_network=[128], activation=\"relu\")\n",
    "\n",
    "    def call_spektral(observations, training=False):\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features (embeddings)\n",
    "        # A: binary adjacency matrix specifying edges in the graph\n",
    "        # E: tensor containg edge features\n",
    "        old_embeddings, A, E = GraphObserver.graph(observations, graph_dims)\n",
    "\n",
    "        # pass the inputs through an edge conditioned convolution\n",
    "        # layer and receive new node embeddings\n",
    "        new_embeddings = edge_convolution([old_embeddings, A, E])\n",
    "\n",
    "        # output the final transformed node embeddings\n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_spektral(observations)\n",
    "    \n",
    "    print(\"Here's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_spektral_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, when `tf2_gnn` is specified, the implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's how the embeddings of the ego agent have changed:\n",
      "\n",
      "old embeddings of shape (5,): \n",
      "[0.53097636 0.7166578  0.14670205 0.15572342 0.95631707]\n",
      "\n",
      "new embeddings of shape (16,): \n",
      "[0.05579451 0.         0.         0.08129452 0.01850684 0.\n",
      " 0.09750077 0.00123036 0.         0.01808714 0.03419167 0.\n",
      " 0.         0.         0.00680932 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from tf2_gnn.layers import GNN, GNNInput\n",
    "\n",
    "def call_tf2_gnn_demo(observations):\n",
    "    # the number and types of layers in the GNN are all encoded\n",
    "    # in the parameters dictionary, let's stick to the default for now\n",
    "    gnn_params = GNN.get_default_hyperparameters()\n",
    "\n",
    "    # uncomment the following two lines to have a look at them\n",
    "    # print(f'GNN parameters:')\n",
    "    # pp.pprint(gnn_params)\n",
    "\n",
    "    # initialize a GNN instance which acts as a keras layer\n",
    "    gnn = GNN(gnn_params)\n",
    "\n",
    "    def call_tf2_gnn(observations, training=False):\n",
    "        batch_size = tf.constant(observations.shape[0])\n",
    "\n",
    "        # convert the observations into\n",
    "        # old_embeddings: tensor containing the node features\n",
    "        # A: dense adjacency list in the format [[0, 1], [2, 4]]\n",
    "        #    specifying source and target node ids of an egde\n",
    "        # node_to_graph_map: a tensor that assigns each node in X to a graph\n",
    "        old_embeddings, A, node_to_graph_map = GraphObserver.graph(\n",
    "          observations,\n",
    "          graph_dims=graph_dims, \n",
    "          dense=True)\n",
    "        \n",
    "        # build the struct that tf2_gnn expects as input\n",
    "        gnn_input = GNNInput(\n",
    "          node_features=old_embeddings,\n",
    "          adjacency_lists=(A,),\n",
    "          node_to_graph_map=node_to_graph_map,\n",
    "          num_graphs=batch_size,\n",
    "        )\n",
    "\n",
    "        new_embeddings = gnn(gnn_input, training=training)\n",
    "        \n",
    "        # only for demo purposes\n",
    "        old_embeddings = tf.reshape(old_embeddings, [batch_size, graph_dims[0], -1])\n",
    "        new_embeddings = tf.reshape(new_embeddings, [batch_size, 5, -1])\n",
    "        \n",
    "        return old_embeddings, new_embeddings\n",
    "    \n",
    "    old_embeddings, new_embeddings = call_tf2_gnn(observations)\n",
    "    \n",
    "    print(\"\\nHere's how the embeddings of the ego agent have changed:\\n\")\n",
    "    print(f'old embeddings of shape {old_embeddings[0, 0].shape}: \\n{old_embeddings[0,0,:].numpy()}\\n')\n",
    "    print(f'new embeddings of shape {new_embeddings[0, 0].shape}: \\n{new_embeddings[0,0,:].numpy()}')\n",
    "\n",
    "# call the function with sample observations\n",
    "call_tf2_gnn_demo(get_sample_observations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the GNN functionality nicely abstracted behind this wrapper, we can now easily integrate it into the Soft-Actor-Critic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Soft-Actor-Critic Algorithm with Graph Neural Networks\n",
    "\n",
    "Next, let's examine the integrated system.\n",
    "\n",
    "We want to exploit the graph-like structure of traffic scenarios and have already encoded the state of the world as a graph. Now, we want to apply graph neural networks to the SAC algorithm. \n",
    "\n",
    "The resulting actor and critic networks are quite similar in structure. Here's how they work and what they compute.\n",
    "\n",
    "### The Actor Network\n",
    "\n",
    "Implemented in the class `GNNActorNetwork`.\n",
    "\n",
    "\n",
    "**Input**: a batch of observations of shape _(batch_size, observation_size)_  \n",
    "**Output**: a batch of a normal distributions over the action space from which the policy will sample the actions performed by the agent\n",
    "\n",
    "![Actor Network Architecture](images/actor_architecture.png)\n",
    "\n",
    "**1. GNN**  \n",
    "The observations are directly fed into the graph neural network (a `GNNWrapper` instance). It converts the observations into graphs and computes new node embeddings for each graph by means of message passing and aggregation. Optionally, the new note embeddings are propagated through a dense layer before being returned.\n",
    "\n",
    "> **NOTE**  \n",
    "From here on, we're only interested in the embeddings of the ego agent. Hence, instead of feeding the whole graph representation into the encoding network, we extract the embeddings of the first node of each graph, which represents the ego agent.\n",
    "\n",
    "**2. Encoding Network**  \n",
    "In the encoding network, the node embeddings of the ego agent are now passed through a series of dense layers. Depending on the parameters passed into the actor, we can also add convolutions, dropout and other types of layers here.\n",
    "\n",
    "**3. Projection Network**  \n",
    "Finally, the projection network receives the hidden representations after the encoding network and computes a normal distribution over the action space for each observation contained in the batch, modeled by a mean and a standard deviation.\n",
    "\n",
    "In a very simplified manner for brevity, the implementation of the actor's `call` function looks as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "def call(self, observations, training=False):\n",
    "    # get the updated node embeddings\n",
    "    output = self._gnn(observations, training=training)\n",
    "\n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the ego agent's node embeddings through the encoder\n",
    "    output = self._encoder(output, training=training)\n",
    "    \n",
    "    # compute a normal distribution\n",
    "    output = self._projection_net(output, training=training)\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "### The Critic Network\n",
    "\n",
    "Implemented in the class `GNNCriticNetwork`.\n",
    "\n",
    "**Input**: a batch of observation-action pairs, i.e. `[obs, action]` with shapes _(batch_size, observation_size)_ and _(batch_size, 2)_  \n",
    "**Output**: a scalar value assigned to each observation-action pair\n",
    "\n",
    "\n",
    "The major difference compared to the actor network is that in the critic, we have two parallel pipelines for the observations and their corresponding actions.\n",
    "\n",
    "![Critic Network Architecture](images/critic_architecture.png)\n",
    "\n",
    "**1. Actions**  \n",
    "The actions are simply passed into an action encoding network that works similar to the encoding network of the actor network, i.e. a series of dense layers with optional convolutions, dropout layers, etc.\n",
    "\n",
    "**2. Observations**  \n",
    "The observations are processed in the exact same way as in the actor network. We compute new graph representations in the GNN, extract the ego node embeddings and pass them through an encoding network.\n",
    "\n",
    "**3. Joining Actions and Observations**  \n",
    "After receiving the outputs from the action and observation encoding networks, we concatenate the observation-action pair of each element in the batch to one feature vector.  \n",
    "Finally, we pass this concatenated state through a fully connected joint network which outputs a scalar value for each observation-action pair.\n",
    "\n",
    "Again, a simplified version of the implemenation looks like this:  \n",
    "```python\n",
    "def call(self, inputs, training=False):\n",
    "    observations, actions = inputs\n",
    "     \n",
    "    # get the updated node embeddings\n",
    "    node_embeddings = self._gnn(observations, training=training)\n",
    "    \n",
    "    # extract the ego state (the first node embedding vector of each batch element)\n",
    "    output = output[:, 0]\n",
    "    \n",
    "    # pass the node embeddings through their observation encoder\n",
    "    node_embeddings = self._observation_encoder(node_embeddings, trainig=training)\n",
    "    \n",
    "    # do the same for the actions with a different action encoder\n",
    "    actions = self._action_encoder(actions, training=training)\n",
    "    \n",
    "    # concatenate observations and actions into one vector\n",
    "    joint = tf.concat([node_embeddings, actions], 1)\n",
    "    \n",
    "    # compute a scalar output value\n",
    "    output = self._joint_net(joint, training=training)\n",
    "\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Putting It All Together and Setting up an Example\n",
    "\n",
    "Now, let's set up an SAC-agent using the graph neural networks described above to be used in BARK-ML.\n",
    "\n",
    "We start out with the default parameter set as defined in `tfa_gnn_params.json` and make some optional changes afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterServer(filename=\"../../examples/example_params/tfa_sac_gnn_spektral_default.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the GNN-related parameters. We use the same GNN configuration in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a spektral GNN\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['Library'] = 'spektral'\n",
    "    \n",
    "# use two message passing layers with 80 channels of node embeddings each\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"NumMpLayers\"] = 2\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"GNN\"][\"MpLayersHiddenDim\"] = 80\n",
    "    \n",
    "# use two fully connected layers in the edge feature mlp of each message passing layer\n",
    "params['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] = [128, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure the layers that make up the encoding networks in the actor and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticJointFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"CriticObservationFcLayerParams\"] = [128, 128]\n",
    "params[\"ML\"][\"BehaviorGraphSACAgent\"][\"ActorFcLayerParams\"] = [256, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the `GraphObserver`.\n",
    "Here we specify that it should always observe at most 4 agents simultaneously, i.e. the ego agent and its three nearest agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ML\"][\"GraphObserver\"][\"AgentLimit\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify which features the graph observer should use in the node embeddings and the edges. This is useful since not all environments contain the same information and thus, some features might not be possible to compute. To get a list of all available features, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available node features:\n",
      "  'x': The x-components of the agent's position.\n",
      "  'y': The y-components of the agent's position.\n",
      "  'theta': The current heading angle of tha agent.\n",
      "  'vel': The current velocity of the agent.\n",
      "  'goal_x': The x-component of the goal's position.\n",
      "  'goal_y': The y-component of the goal's position.\n",
      "  'goal_dx': The difference in the x-component of the agent's and the goal's position.\n",
      "  'goal_dy': The difference in the y-component of the agent's and the goal's position.\n",
      "  'goal_theta': The goal heading angle.\n",
      "  'goal_d': The euclidian distance of the agent to the goal.\n",
      "  'goal_vel': The goal velocity.\n",
      "\n",
      "Available edge features:\n",
      "  'dx': The difference in the x-position of the two agents.\n",
      "  'dy': The difference in the y-position of the two agents.\n",
      "  'dvel': The difference in the velocity of the two agents.\n",
      "  'dtheta': The difference in the heading angle of the two agents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available node features:\")\n",
    "for key, value in GraphObserver.available_node_attributes(with_descriptions=True).items():\n",
    "    print(f\"  '{key}': {value}\")\n",
    "\n",
    "print(f\"\\nAvailable edge features:\")\n",
    "for key, value in GraphObserver.available_edge_attributes(with_descriptions=True).items():\n",
    "    print(f\"  '{key}': {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we train an agent on the `ContinuousMergingBlueprint` where the goal definition does not contain velocity information, so we can not use the `goal_vel` node feature. So let's configure the `GraphObserver` to use all available node features, except `goal_vel`.\n",
    "\n",
    "In the edges, we want to use all available features, so we don't specify anything. The `GraphObserver` always defaults to using all features if nothing is explicitely configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_node_features = GraphObserver.available_node_attributes()[:-1]\n",
    "params[\"ML\"][\"GraphObserver\"][\"EnabledNodeFeatures\"] = enabled_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you feel like experimenting, expand the following dropdown to get an overview of the most important parameters you can tweak.  \n",
    "> **NOTE**  \n",
    "The dropdown is not visible when viewing the notebook on GitHub.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>List of the most important paramaters</b></summary>\n",
    "<br>\n",
    "  <b>Description:</b> Specifies the maximum number of agents that are included in an observation. (int)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['AgentLimit'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies whether each node in the graph will have an edge pointing to itself. (Bool)<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['SelfLoops'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the node embeddings. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledNodeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the features that the GraphObserver will include in the edges. [str]<br>\n",
    "  <b>Path:</b> ['ML']['GraphObserver']['EnabledEdgeFeatures'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the actor encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['ActorFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic action encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticActionFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic observation encoding network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticObservationFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) of the critic joint network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['CriticJointFcLayerParams'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['NumMpLayers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies the number of units in the message passing layers in the GNN. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MpLayersHiddenDim'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies which library to use as the GNN implementation, either \"tf2_gnn\" or \"spektral\". <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['Library'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to TF2-GNN.</h3>\n",
    "\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a relational gated convolution network. (str)\n",
    "      <br><i>NOTE: when using the 'ggnn' message passing layer, 'MpLayersHiddenDim' must match the number of node features!</i> <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.message_calculation_class'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> The identifier of the message passing class to be used, here: a gated recurrent unit. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['global_exchange_mode'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a dense layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['dense_every_num_layers'] <br>\n",
    "  <br>\n",
    "  <b>Description:</b> Specifies after how many message passing layers a global exchange layer is inserted. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN.global_exchange_every_num_layers'] <br>\n",
    "  \n",
    "  <h3>The following parameters only apply to Spektral.</h3>\n",
    "\n",
    "  <b>Description:</b> Specifies the number of channels in the edge conditioned convolution layer. (int) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPChannels'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the fully connected layers (number and sizes) in the edge network. ([int]) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['EdgeFcLayerParams'] <br>\n",
    "  \n",
    "  <b>Description:</b> Specifies the activation function of the message passing layer. (str) <br>\n",
    "  <b>Path:</b> ['ML']['BehaviorGraphSACAgent']['GNN']['MPLayerActivation'] <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure the BARK-ML environment and our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params, number_of_senarios=1, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)\n",
    "    \n",
    "# create agent\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAGENT SUMMARY\u001b[0m\n",
      "\n",
      "Network                        Parameters\n",
      "==========================================\n",
      "ActorNetwork...................... 547.300\n",
      "CriticNetwork..................... 553.441\n",
      "CriticNetwork2.................... 553.441\n",
      "TargetCriticNetwork1.............. 553.441\n",
      "TargetCriticNetwork2.............. 553.441\n",
      "------------------------------------------\n",
      "Total parameters                 2.761.064\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import prepare_agent, summarize_agent\n",
    "\n",
    "# only for demo purposes\n",
    "prepare_agent(agent, params, env)\n",
    "summarize_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Verify the Actor Network with Supervised Learning\n",
    "In this part, we briefly introduce a supervised learning setting which helps to quickly debug different actor implementations. It is evaluated whether the actor network is capable of overfitting a small dataset by comparing it to a `RandomActor` and a `ConstantActor`. The `RandomActor` simply outputs a random label within a prespecified bound. The `ConstantActor` on the other hand always outputs the mean label of the training dataset. This chapter demonstrates what we've implemented in `py_gnn_actor_tests`.\n",
    "\n",
    "Additionally, the performance while learning is compared to the standard SAC actor for better comparison with the help of TensorBoard.\n",
    "\n",
    "But first, let's define a few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for default parameter files\n",
    "filename_tf2_gnn = \"../../examples/example_params/tfa_sac_gnn_tf2_gnn_default.json\"\n",
    "filename_spektral = \"../../examples/example_params/tfa_sac_gnn_spektral_default.json\"\n",
    "filename_normal = \"../../examples/example_params/tfa_params.json\"\n",
    "\n",
    "params_tf2_gnn = ParameterServer(filename=filename_tf2_gnn)\n",
    "params_spektral = ParameterServer(filename=filename_spektral)\n",
    "params_normal = ParameterServer(filename=filename_normal)\n",
    "\n",
    "num_scenarios = 3\n",
    "log_dir = \"supervised/summary\"\n",
    "data_path = \"supervised/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the different actors for benchmarking and fetch or load a small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# get an observer and an actor net configured with Spektral\n",
    "observer, spektral_actor = configurable_setup(params_spektral, num_scenarios=num_scenarios);\n",
    "\n",
    "# get an actor net configured with tf2_gnn\n",
    "_, tf2_gnn_actor = configurable_setup(params_tf2_gnn, num_scenarios=num_scenarios);\n",
    "\n",
    "# get a normal SAC actor (without GNNs)\n",
    "_, normal_sac_actor = configurable_setup(params_normal, num_scenarios=num_scenarios, graph_sac=False);\n",
    "\n",
    "# get a random actor (outputs random labels with uniform distribution within bounds)\n",
    "random_actor = RandomActorNet(low=[0, -0.4], high=[0.1, 0.4])\n",
    "\n",
    "# construct a supervised dataset using the observer as data generator\n",
    "dataset = SupervisedData(observer, params_tf2_gnn, batch_size=32, train_split=0.8,\n",
    "                         data_path=data_path, num_scenarios=num_scenarios);\n",
    "\n",
    "# get a constant actor (outputs constant mean labels of train_dataset)\n",
    "constant_actor = ConstantActorNet(dataset=dataset._train_dataset)\n",
    "\n",
    "\n",
    "actors = {\n",
    "    \"tf2_gnn_actor\": {\"actor\": tf2_gnn_actor},\n",
    "    \"spektral_actor\": {\"actor\": spektral_actor},\n",
    "    \"normal_sac_actor\": {\"actor\": normal_sac_actor},\n",
    "    \"random_actor\": {\"actor\": random_actor},\n",
    "    \"constant_actor\": {\"actor\": constant_actor}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the magic starts.\n",
    "All trainable actors are trained (the RandomActor and ConstantActor are not trainable) for a few epochs. The results of the training can then be examined in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marco.oliva/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Delete all old logs if some exist\n",
    "clean_log_dir(log_dir)\n",
    "old_logs = []\n",
    "\n",
    "# Run benchmarking\n",
    "for actor_name in actors:\n",
    "    loss = benchmark_actor(actors[actor_name][\"actor\"], dataset, epochs=100, log_dir=log_dir)\n",
    "    actors[actor_name][\"loss\"] = loss\n",
    "    \n",
    "    # Name log clearly with actor_name\n",
    "    # Select correct log\n",
    "    new_logs = os.listdir(log_dir)\n",
    "    log = list(set(new_logs) - set(old_logs))[0]\n",
    "    \n",
    "    # Rename log with actor_name\n",
    "    old_path = os.path.join(log_dir, log)\n",
    "    new_path = os.path.join(log_dir, actor_name)\n",
    "    os.rename(old_path, new_path)\n",
    "    old_logs = os.listdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5d60335e9be30769\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5d60335e9be30769\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir supervised/summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Train an actual Agent using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's finally train an agent on a BARK-ML environment.\n",
    "\n",
    "To be able to monitor the training, we launch a TensorBoard instance first. For now, there's nothing to see there, but once we start the training, it will reload and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-215e251e449a83a6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-215e251e449a83a6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_log_dir(\"training/sac_gnn_spektral\")\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir training/sac_gnn_spektral/summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a fresh setup of the environment and the agent so that we have it all in one place. Remember that you can customize the agent just as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Set a name for this run to recognize it in TensorBoard.\n",
    "# When training multple times, make sure to set different names, so that the logs are not overwritten.\n",
    "# By default, we use a simple timestamp.\n",
    "training_run_name = str(datetime.datetime.now())\n",
    "\n",
    "# Define how many iterations you want to run (recommended for actual training is >= 40000).\n",
    "num_training_iterations = 200\n",
    "\n",
    "# load default parameters\n",
    "params = ParameterServer(filename='data/tfa_sac_gnn_spektral_default.json')\n",
    "params[\"ML\"][\"BehaviorTFAAgents\"][\"CheckpointPath\"] += f\"/{training_run_name}\"\n",
    "params[\"ML\"][\"TFARunner\"][\"SummaryPath\"] += f\"/{training_run_name}\"\n",
    "params[\"ML\"][\"SACRunner\"][\"NumberOfCollections\"] = num_training_iterations\n",
    "params[\"ML\"][\"GraphObserver\"][\"EnabledNodeFeatures\"] = [\"x\", \"y\", \"vel\", \"theta\"]\n",
    "\n",
    "# create environment\n",
    "bp = ContinuousMergingBlueprint(params, number_of_senarios=2500, random_seed=0)\n",
    "observer = GraphObserver(params=params)\n",
    "env = SingleAgentRuntime(blueprint=bp, observer=observer, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3a61adf9583a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# start the example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_rl_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/gnn_practial_course_2020/run.runfiles/bark_ml/docs/gnn_practial_course_2020/helper_functions.py\u001b[0m in \u001b[0;36mrun_rl_example\u001b[0;34m(env, agent, params, mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetupSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"visualize\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/gnn_practial_course_2020/run.runfiles/bark_ml/bark_ml/library_wrappers/lib_tf_agents/runners/tfa_runner.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/gnn_practial_course_2020/run.runfiles/bark_ml/bark_ml/library_wrappers/lib_tf_agents/runners/sac_runner.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_training_duration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/agents/sac/sac_agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    321\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m       alpha_loss = self._alpha_loss_weight*self.alpha_loss(\n\u001b[0;32m--> 323\u001b[0;31m           time_steps, weights=weights)\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_numerics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Alpha loss is inf or nan.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0malpha_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/agents/sac/sac_agent.py\u001b[0m in \u001b[0;36malpha_loss\u001b[0;34m(self, time_steps, weights)\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0munused_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actions_and_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m       \u001b[0mentropy_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_pi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0malpha_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_alpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/agents/sac/sac_agent.py\u001b[0m in \u001b[0;36m_actions_and_log_probs\u001b[0;34m(self, time_steps)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     action_distribution = self._train_policy.distribution(\n\u001b[0;32m--> 399\u001b[0;31m         time_steps, policy_state=policy_state).action\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;31m# Sample actions and log_pis from transformed distribution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[0;32m--> 149\u001b[0;31m         network_observation, time_step.step_type, policy_state, mask=mask)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[0;34m(self, observation, step_type, policy_state, mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       return self._actor_network(\n\u001b[0;32m--> 124\u001b[0;31m           observation, step_type, policy_state, training=self._training)\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       return self._actor_network(\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/tmp/_bazel_marco.oliva/818820b99828947bd7cbf1a2080eafc5/execroot/bark_ml/bazel-out/darwin-fastbuild/bin/docs/gnn_practial_course_2020/run.runfiles/bark_ml/bark_ml/library_wrappers/lib_tf_agents/networks/gnn_actor_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# extract ego state (node 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Development/bark-ml/python/venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helper_functions import run_rl_example\n",
    "\n",
    "agent = BehaviorGraphSACAgent(environment=env, observer=observer, params=params)\n",
    "\n",
    "# start the example\n",
    "run_rl_example(env, agent, params, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it actually look? \n",
    "\n",
    "After training the agent for around 40k iterations on the `ContinuousMergingBlueprint`, it has converged to a mean reward of around 1. It's behavior then looks as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/mrcoliva/bark-ml/raw/master/docs/gnn_practial_course_2020/images/gnn-sac-merging-bp.gif\" alt=\"BARK-ML Highway\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix: Commands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can run the example from the command line with `bazel run //examples:tfa_gnn -- --mode=train`.  \n",
    "The process can be visualized using TensorBoard with `tensorboard --logdir training/sac_gnn_spektral/summaries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
