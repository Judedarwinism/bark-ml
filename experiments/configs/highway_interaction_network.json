{
    "Experiment": {
        "Observer": {
            "ModuleName": "GraphObserver",
            "Config": {}
        },
        "Evaluator": {
            "ModuleName": "GoalReachedGuiding",
            "Config": {}
        },
        "Runtime": {
            "ModuleName": "SingleAgentRuntime",
            "Config": {}
        },
        "Runner": {
            "ModuleName": "SACRunner",
            "Config": {}
        },
        "Agent": {
            "ModuleName": "BehaviorGraphSACAgent",
            "Config": {
                "init_gnn": "init_interaction_network"
            }
        },
        "Blueprint": {
            "ModuleName": "ContinuousHighwayBlueprint",
            "Config": {
                "num_scenarios": 10000,
                "viewer": true
            }
        },
        "NumEvaluationEpisodes": 500,
        "NumVisualizationEpisodes": 10
    },
    "Visualization": {
        "Agents": {
            "Color": {
                "Other": {
                    "Lines": [
                        0.7,
                        0.7,
                        0.7
                    ],
                    "Face": [
                        0.7,
                        0.7,
                        0.7
                    ]
                },
                "Controlled": {
                    "Lines": [
                        0.0,
                        0.27,
                        0.58
                    ],
                    "Face": [
                        0.49,
                        0.63,
                        0.83
                    ]
                },
                "UseColormapForOtherAgents": false,
                "IfColormapUseLineColorOthers": true
            },
            "Alpha": {
                "Controlled": 1.0,
                "Other": 1
            },
            "ColorRoute": [
                0.2,
                0.2,
                0.2
            ],
            "DrawRoute": false,
            "DrawAgentId": true,
            "DrawEvalGoals": true,
            "EvalGoalColor": [
                0.49,
                0.63,
                0.83
            ],
            "DrawHistory": false,
            "DrawHistoryDrawFace": true
        },
        "Map": {
            "XodrLanes": {
                "Boundaries": {
                    "Color": [
                        0.7,
                        0.7,
                        0.7
                    ],
                    "Alpha": 1.0,
                    "Linewidth": 1.0
                }
            },
            "Plane": {
                "Color": [
                    1,
                    1,
                    1,
                    1
                ],
                "Alpha": 1.0
            }
        },
        "Evaluation": {
            "DrawLTLDebugInfo": false
        }
    },
    "ML": {
        "BehaviorTFAAgents": {
            "CheckpointPath": "/Users/hart/Development/bark-ml/experiments/runs/gcn_three_layers/ckpts/",
            "NumCheckpointsToKeep": 3
        },
        "TFARunner": {
            "SummaryPath": "/Users/hart/Development/bark-ml/experiments/runs/gcn_three_layers/summ/",
            "EvaluationSteps": 25,
            "InitialCollectionEpisodes": 50,
            "CollectionEpisodesPerStep": 1
        },
        "GoalReachedEvaluator": {
            "GoalReward": 1.0,
            "CollisionPenalty": -1.0,
            "MaxSteps": 60
        },
        "NearestObserver": {
            "NNearestAgents": 4,
            "MinVel": 0.0,
            "MaxVel": 50.0,
            "MaxDist": 75.0,
            "StateSize": 4
        },
        "BehaviorContinuousML": {
            "ActionsLowerBound": [
                -5.0,
                -0.2
            ],
            "ActionsUpperBound": [
                4.0,
                0.2
            ]
        },
        "StateObserver": {
            "VelocityRange": [
                0,
                100
            ],
            "ThetaRange": [
                -6.283185307179586,
                6.283185307179586
            ],
            "NormalizationEnabled": true,
            "MaxNumAgents": 2
        },
        "GraphObserver": {
            "NormalizationEnabled": true,
            "AgentLimit": 4,
            "VisibilityRadius": 1500,
            "SelfLoops": true,
            "EnabledNodeFeatures": [
                "x",
                "y",
                "theta",
                "vel"
            ],
            "EnabledEdgeFeatures": [
                "dx",
                "dy"
            ]
        },
        "BehaviorGraphSACAgent": {
            "ActorFcLayerParams": [
                256,
                256
            ],
            "CriticObservationFcLayerParams": null,
            "CriticActionFcLayerParams": null,
            "CriticJointFcLayerParams": [
                256,
                256
            ],
            "ActorLearningRate": 0.0003,
            "CriticLearningRate": 0.0003,
            "AlphaLearningRate": 0.0003,
            "TargetUpdateTau": 0.05,
            "TargetUpdatePeriod": 3,
            "Gamma": 0.995,
            "RewardScaleFactor": 1.0,
            "AgentName": "gnn_sac_agent",
            "DebugSummaries": true,
            "ReplayBufferCapacity": 10000,
            "ParallelBufferCalls": 1,
            "BatchSize": 512,
            "BufferNumSteps": 2,
            "BufferPrefetch": 3
        },
        "SACRunner": {
            "NumberOfCollections": 250000,
            "EvaluateEveryNSteps": 500
        }
    },
    "BehaviorDynamicModel": {
        "IntegrationTimeDelta": 0.05000000074505806
    },
    "World": {
        "remove_agents_out_of_map": true
    }
}